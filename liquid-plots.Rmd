---
title: "LiquidPlots"
output:
  html_document:
    df_print: paged
    code_folding: hide
---

Author: C0dingM0nk3y    [>>GitHub link<<](https://github.com/C0dingM0nk3y)

---

# {.tabset}

[] Add descriptions\ 
[] Change order of downloads\ 
[x] Improve coin filter\ 
[] Wrap ref data into zip file\
[] Claim from start\


```{r general-options, code_folding="none"}
#RUN OPTIONS
#limit downlod to specific sublist (for troubleshooting purposes)
subset_list <- c("BTC|ETH|STG|RUNE|DOGE") #use REGEX syntax 


#toggle TRUE/FALSE to skip some parts of this script
run_api = FALSE # dowload data/usel local
  subset_download = TRUE #FALSE = use full data
run_analysis = TRUE
  subset_analysis = TRUE #FALSE = use full data
run_plots = TRUE
```

```{r plot-options }
#ANALYSIS OPTIONS
temp = 1
```


```{r init, include=FALSE}
#If not installed yet, install pacman package manager by "uncommenting" the line below

#install.packages("pacman") #this is only required on first run.

source("_SCRIPTS/init_externalLibs.R") 
#If not installed yet, pacman will download and install all required packages on the first run of the script.

source("_SCRIPTS/init_functions.R")
source("_SCRIPTS/init_plots.R")
source("_SCRIPTS/init_dirs.R")
```

---

## Part 1: DATA-DOWNLOAD

---

### API address and credentials
Import USER_KEY from *"/credentials.txt"*\
Use format and save as into [root dir] as *'credentials.txt'*:\
<p style="margin-left: 40px">
key;YOUR-BINANCE-API-KEY\
prKey;YOUR-PRIVATE-KEY\
</p>

```{r credentials, results = "asis"}
#API address
API_root <- "https://api.binance.com" 
  
if(run_api){
  myPrivatePath <- "D:/Clouds/Dropbox/Everywhere/PROJECTS/PiggyBank/Binance/liquid-plot_myCredential.txt" #TEMP
  
  #Import credentials
  #keys <- read.csv2("credentials.txt", col.names = c("Type", "Key"), header = F)
  keys_import <- read.csv2(myPrivatePath, col.names = c("Type", "Key"), header = F)
  
  key_public <- keys_import[1,"Key", drop=T]
  key_private <- keys_import[2,"Key", drop=T]
  rm("keys_import")
  
  #USER FEEDBACK
  if(key_public=="YOUR-BINANCE-API-KEY"){
    stop("STOP: add your Binance API credential to /credential.txt (; separated)")
  }
}
```

Data will be recovered from [`r API_root`] using the following credentials:\
<p style="margin-left: 40px">
**PUBLIC key**: `r ifelse(run_api, key_public, "SKIPPED")`\
**PRIVATE key**: [see "/credentials.txt"]
</p>

---

### DOWNLOAD Binance Data [API]

for info, refer to: https://binance-docs.github.io/apidocs/spot/en/#change-log\
Download to: `r dir.IN.single`\
Download to: `r dir.IN`\

***
#### GET: PRICES
Latest SPOT prices from Binance. Used for HODL vs. LP calculation\

```{r price, results="hold"}
# DOWNLOAD PRICES
if(run_api){
  API_query <- "/api/v3/ticker/price"
  
  price.j.path <- paste0(dir.IN, "API_price.json")
  
  price.j <- BINANCE.GET(API_root, "/api/v3/ticker/price") #download Binance latest Prices
  
  write_json(price.j, price.j.path, auto_unbox=TRUE) #export
  cat("\t-> saved to: ", price.j.path, "\n")
  
  price.df <- fromJSON(price.j.path, simplifyVector = TRUE,
                     flatten = TRUE) #automatically UN-Nest nested columns
  
  #append price datastamp (from file last edit)
  timestamp <- file.info(price.j.path)$mtime %>% as.POSIXct(tz="UTC") %>% 
    round(0) #this is to remove msec from time
  price.df[,"timestamp_UTC"] <- as.character(timestamp)
  
  write.csv2(price.df, file = str_replace(price.j.path, pattern = ".json", "_unpacked.csv"), row.names = F)
  cat(sprintf("\tCONVERTED TO .CSV\t-> export to: %s (%s rows)\n", 
              str_replace(price.j.path, pattern = ".json", "_unpacked.csv"), nrow(price.df)))
}else{cat("SKIPPED\n")}
```

***
#### GET: LIQUIDITY POSITIONS (Currently active)
Latest data about currently active pool and their Coin composition. 

```{r liquidity-now, results="hold"}
if(run_api){
  # DOWNLOAD
  liq.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/liquidity", timestamp = TRUE, sign = TRUE) 
  
  # JSON EXPORT
  liq.j.path <- paste0(dir.IN, "API_liq.json")
  write_json(liq.j, liq.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS
  cat(sprintf("\t-> export to: %s\n", liq.j.path))
  
  # TABLE INTERPRETATION
  #> this function does not ONLY unpack liquidity data, but it also unpivot the table, to make it readable
  liq.df <- liquidity.tableInterpreter(liq.j.path) 

  if(subset_download){
    message("FITER APPLIED: only selected pools are kept")            #TEMP
    liq.df %<>% subset(grepl(subset_list, liq.df$poolName))}  #TEMP
 
  
  liq.df.path <- paste0(dir.IN, "API_liquidity_unpacked.csv")
  write.csv2(liq.df, liq.df.path, row.names = F)
  cat(sprintf("\tUNPACKED INTO .CSV\t-> export to: %s (%s rows)\n", liq.df.path, nrow(liq.df)))
  
}else{cat("SKIPPED\n")}
```

#### PLOT: ACTIVE POOLS
found `r ifelse(run_api, length(unique(liq.df$poolName)), "X")` pools:

```{r active-pools}
if(run_api){
  
  liq.df[,"Date_UTC"] <- msec_to_datetime(liq.df$updateTime)
  
  #Print current Pools
  liq.df %>% 
    subset(select = c("poolName", "poolId", "Date_UTC", "share.Amount")) %>% #select useful col.
    unique() %>% #filters out duplicated entries (2x for each pool)
    kable(align= "c", caption = "<b>Currently ACTIVE Pools</b>") %>%
    kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F) %>%
    column_spec(c(1), bold=T) 
  
}else{cat("SKIPPED\n")}
```

---

#### GET: HISTORICAL POOLDATA
This includes Pool Operations (ADD/REMOVE) and Rewards Claims
_only data from the pools above is downloaded_


```{r api-singlePools, results="hold"}
# MAX trx x pool is 100. Use pagination if more is required (may be required for Claims)

max_iter = 10 #so far, only applies to Claimed. operations does not requires that

if(run_api){

  poolId_list <- unique(liq.df$poolId) #all unique poolId
  poolNames_list <- unique(liq.df$poolName) #unique poolNames

  
  for (n in 1:length(poolId_list)){
    #POOL-NAME
    id <- poolId_list[n]
    pName <- poolNames_list[n]
    
    cat(sprintf("\n\tPool: %s (poolId=%s)\n", pName, id))
    
    #POOL OPERATIONS
    ops.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/liquidityOps", 
                       API_param = paste0("limit=100&poolId=",id),
                       timestamp = TRUE, sign = TRUE) 
    
    ops.j.path <- paste0(dir.IN.single, id, "_ops.json")
    write_json(ops.j, ops.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS/SinglePools/
    
    #CLAIMED DATA
    claim.j_list <- list() #for data collection
    
    # first ITER
    claim.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/claimedHistory", 
                       API_param = paste0("type=1&limit=100&poolId=",id), #type 0/1 = pending/successful
                       timestamp = TRUE, sign = TRUE) 
    claim.j_list[1] <- list(claim.j) #claim.j is already a list (1:100). This step creates a list of list.
    
    # more ITER, if required
    for (n in 1:max_iter){
      if (max_iter==0){break} #allow to break loop in case this is desirable (e.g. quick update)
      
      last_claim <- claim.j_list[[n]]
      if (length(last_claim)==100){ #that means that data reached limit (100) and need a separate API call to prev. one
        lastTimestamp <- last_claim[[100]]$claimedTime #recovers last claimed time [[100]] from last list [[n-1]]
        
        cat(paste0("\titer=",n+1,"\t")) #add an indentation and iter number to export      
        claim.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/claimedHistory", timestamp = TRUE, sign = TRUE,
                         API_param = paste0("type=1&limit=100&poolId=",id, "&endTime=", lastTimestamp)) 
                    #Why lastTimestamp and NOT lastTimestamp-1
                    # > each time stamps has 2-3 rows (one for each coin + extra rewards)
                    # > there is 2/3 chance that last row of table is incomplete, and miss at least one of the coin. 
                    # > therefore, it is better to download that again, and the remove duplicate entries in later steps
        
        claim.j_list[n+1] <- list(claim.j) #list of list (JSON is 1-100 list)
      }
      else{break} #break loop: last claim had less than 100 entries
    }
    
    # List JOIN and EXPORT
    claim.joined <- unlist(claim.j_list, recursive = F)
    
    claim.j.path <- paste0(dir.IN.single, id, "_claim.json")
    write_json(claim.joined, claim.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS/SinglePools/
    if (length(claim.joined)>100){cat(sprintf("\t\t joined %s entries into single file: %s\n", length(claim.joined), claim.j.path))}
  }
  
}else{cat("SKIPPED\n")}
```


```{r cleanup1}
var_all <- ls()
var_keep <- c(grep(var_all, pattern = ".df|dir|DF|run|API|key|full|subset", value=T),
              lsf.str()) # all functions names, also considered variables
var_rm <- setdiff(var_all,var_keep)              
rm(list=c(var_rm, "var_all", "var_keep", "var_rm"))
```

## Part 2: DATA ANALYSIS

TO IMPLEMENT\
[] single export data analysis (collect all data in single file) [BIG: requirs data restructing]\
[] historical data analysis (results from previous LP positions)\

```{r analysis-init}
# VARIABLES and DF that need to be re-imported before running Part 2

if(run_api==FALSE){
  price.df <- read.csv2(paste0(dir.IN,"API_price_unpacked.csv"))
  liq.df <- read.csv2(paste0(dir.IN,"API_liquidity_unpacked.csv"))
 
  if(subset_analysis){
    message("FILTER APPLIED: only selected pools are kept")            #TEMP
    liq.df %<>% subset(grepl(subset_list, liq.df$poolName))}  #TEMP
  
   }
  
# Useful lists
poolId_list <- unique(liq.df$poolId) #all unique poolId
poolNames_list <- unique(liq.df$poolName) #unique poolNames
  
# unique coins (to fetch prices)
coinList <- poolNames_list %>% str_split("/") %>% unlist() %>% 
  c("BNB") %>% #add BNB, as this will be needed for extraRewards calculations
  unique()
```

### Price Matrix
Extract prices from latest API [`r price.df[1,"timestamp_UTC"]`]:\

```{r price-matrix, include=FALSE}
refCoin = "USDT"

if(run_analysis){
  # EXTRACTS PRICES for the relevant coins
  priceMatrix <- data.frame()
  
  for (c in coinList){
    priceMatrix[c,refCoin] <- getPrice(price.df, c, refCoin = refCoin) %>% as.numeric()}

}else{cat("SKIPPED\n")}
```

All prices are expressed in **`r refCoin`**

#### Active Pools DATA
Overview of latest pools [currently active]

>>>THIS BELOW SHOULD GO INTO A FUNCTION<<<

```{r active-calc}
#> Unpivot active-pools
#> Append current prices
#> Calculate values
 
if(run_analysis){
  # Assign price value to each coin
  liq.df[,"Price"] <- priceMatrix[liq.df$Coin, refCoin] #recover price from priceMatrix
  liq.df[,"Value"] <- with(liq.df, share.asset*Price) %>% round(8)
  liq.df[,"Currency"] <- refCoin
  
  # perfrom all CALCULATION in separate FUNC
  active.DF <- activePools.Calc(liq.df)
  
  # EXPORT
  write.csv2(active.DF, paste0(dir.TABLES, "ActivePools.csv"), row.names = F)
  
  # USER PLOT
  # Print current Pools
  active.DF %>% subset(select = c("poolName", "poolId", "Date_UTC", 
                                  "Qnt1", "Coin1", "Qnt2", "Coin2", 
                                  "Value_TOT", "Currency")) %>% #select useful col.
    kable(digits = c(2,2,2,4,4,4,4,2,2), align= c("c","c","r","r","l","r","l","r","l"), 
          caption = "<b>Currently ACTIVE Pools</b>") %>%
    kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F) %>%
    column_spec(c(4,5), color="darkblue") %>% 
    column_spec(c(1,8,9), bold=T) 
  

}else{cat("SKIPPED\n")}
```

#### Database Update (SINGLE pools)

For each pool that is currently active:
- Import JASON from "/DOWNLOAD/SinglePools/"\
- Export data as _unpivoted.csv into "/TABLES/SinglePools/"\
- Finally, check if previous data are available and, if so, append new data (HISTORY)\

> [ ] Consider wrapping the module below into a function. *Or also, not*

```{r analysis-ops}
# Import JASON from "/DOWNLOAD/SinglePools/"
# Export data as unpivoted .csv into "/TABLES/SinglePools/" (Claim ONLY)
# OPS and SNAPSHOTS have the same structure, they are joined together into a single file (poolHistory.csv)
# Finally, check if previous data are available and, if so, append new data (HISTORY)

if(run_analysis){
  
  for(id in poolId_list){
    
    cat(sprintf("Exporting (poolId=%s) to /TABLES/SinglePools/:", id))
    # PATHS
    ops.j.path <- paste0(dir.IN.single,id,"_ops.json")
    ops.table.path <- paste0(dir.TABLES.single,id,"_ops.csv") #REMOVE after making sure there is conflict
    claim.j.path <- paste0(dir.IN.single,id,"_claim.json")
    claim.table.path <- paste0(dir.TABLES.single,id,"_claim.csv")
    #snap.table.path <- paste0(dir.TABLES.single,id,"_snapshot.csv")
    #snap.history.path <- paste0(dir.TABLES.single,id,"_snapshots.History.csv") #REMOVE after first run
    pool.history.path <- paste0(dir.TABLES.single,id,"_pool.History.csv")
    
    # CLAIMED DATA
    claim_DF <- claim.tableInterpreter(claim.j.path)
    write.csv2(claim_DF, claim.table.path, row.names = F) # EXPORT
    cat(sprintf("\t _claim.csv (%s rows)", nrow(claim_DF)))
    
    # INTEPRET ops table
    ops_DF <- ops.tableInterpreter(ops.j.path)
    write.csv2(ops_DF, ops.table.path, row.names = F) # EXPORT #REMOVE?
    cat(sprintf("\t _ops.csv (%s rows)", nrow(ops_DF))) #REMOVE?
    
    # Last Pool Balance (SNAPSHOT)
    
    snap_DF <-  subset(active.DF, poolId==id)
    #add missing columns before joining to ops table
    snap_DF[,"shareAmount"] <- snap_DF[,"share.Amount"] #name change
    snap_DF[,"operation"] <- "SNAPSHOT"
    
    # Update/Crete HISTORY file (HISTORY)
    if (file.exists(pool.history.path)){
      pool_prev <- read.csv2(pool.history.path) #import prev history
      #recalculate from Unix, that is always the safest option 
      #> reason: if the file was opened with Excel, it changes all data formats)
      pool_prev[, "Date_UTC"] <- msec_to_datetime(pool_prev$Date_Unix, tz = "UTC")} 
    else{pool_prev <- snap_DF} #...or replace it with latest data
    
    # JOIN and removes duplicates
    common_cols <- intersect(colnames(ops_DF), colnames(pool_prev)) #why? because snap_DF has some extra cols with Values, and I want to drop them
    pool_HIST <- bind_rows(list(ops_DF, #operations (OLD and NEW) 
                                pool_prev[,common_cols], # OLD DATA (including both OPS and SNAPS)
                                snap_DF[,common_cols])) # LAST SNAPSHOT (always NEW)
    pool_HIST %<>% unique() #removes duplicates
    pool_HIST <- pool_HIST[order(pool_HIST$Date_Unix), ] #order chronologically
  
    write.csv2(pool_HIST, pool.history.path, row.names = F) # EXPORT HIST file
    cat(sprintf("\t _pool.History.csv (read:%s, write:%s rows)\n", nrow(pool_prev), nrow(pool_HIST)))
  }
}
```


## Part 3: PLOTS

TO IMPLEMENT\
[x] automated plotting\
[x] link to Binance pool\
[x] link to source data\
[x] color coded tabs\
[x] Subgroups\
[x] Plot margins\
[ ] Summary table (Analysis part)\
                
### Plots.byPool{.tabset}
Click on tab below to select the pool to visualize

***

```{r overview, include=FALSE}
poolId_list_original <- poolId_list

summary.DF <- data.frame(active.DF, row.names = active.DF$poolId) 

for (i in poolId_list_original){
  pool.history.path <- paste0(dir.TABLES.single,i,"_pool.History.csv") #includes OPS and SNAPS
  #claim.table.path <- paste0(dir.TABLES.single,i,"_claim.csv")
  
  pool_H <- read.csv2(pool.history.path) #%>% as.tibble()
  pool_H[,"Date_UTC"] %<>% as.POSIXct(tz="UTC")
  #claim_H <- read.csv2(claim.table.path) #REPLACE WITH HIST FILE
  #claim_H[,"Date_UTC"] %<>% as.POSIXct(tz="UTC")

  start_DF <- pool_H %>% 
    subset(operation=="ADD") %>% 
    tail(1)# find last ADD
  
  summary.DF[as.character(i), "startDate"] <- start_DF[1, "Date_UTC", drop=T]
}

summary.DF <- summary.DF[order(summary.DF$Value_TOT, decreasing = T), ]

#LISTS
list_higVal <- active.DF[active.DF$Value_TOT>100, ]$poolId 
list_longTime <- active.DF[now()-summary.DF$startDate>30, ]$poolId 
list_rest <- poolId_list_original %>%
  setdiff(list_higVal) %>%
  setdiff(list_longTime)
```

#### Subgroups{.tabset}
somethings

##### Group 1{.tabset}
Pools to monitor\

```{r plots1, results='asis', fig.align='center', fig.height=6, fig.width=10}

setStopLoss <- 0.005 #set desired stop loss for plots

allow_refData = FALSE

for (id in list_higVal){
  source("_SCRIPTS/script_plotPool_v0.R") # all plot calc
  }
```

##### Group 2{.tabset}
Pools that are active since a long time

```{r plots2, results='asis', fig.align='center', fig.height=6, fig.width=10}

setStopLoss <- 0.005 #set desired stop loss for plots

allow_refData = FALSE

for (id in list_longTime){
  source("_SCRIPTS/script_plotPool_v0.R") # all plot calc
  }
```


##### Group 3{.tabset}
Other plots

```{r plots3, results='asis', fig.align='center', fig.height=6, fig.width=10}
setStopLoss <- 0.01 #set desired stop loss for plots

allow_refData = FALSE

for (id in list_rest){
  source("_SCRIPTS/script_plotPool_v0.R") # all plot calc
  }
```

#### Stop-loss param{.tabset}
Effect of stop-loss parameter \
_note: this applies only to on a subset of pools (max 3)_

```{r plots4, results='asis', fig.align='center', fig.height=6, fig.width=10}
allow_refData = FALSE

for (sl in c(0.001, 0.005, 0.01)){
  setStopLoss <- sl #set desired stop loss for plots
  
  cat(sprintf("##### max %s%% loss<br>\n", round(setStopLoss*100,2)))
  
  for (id in list_higVal[1:3]){
    source("_SCRIPTS/script_plotPool_v0.R") # all plot calc
    cat("***\n")
    cat("\n")
  }
}

```

***
***

### .footnotes
Notes on PNL vs. HODL plots: \
_why does IL/IG looks different on the 2 plots?_\
<ul>
  <li>
  PNL (USDT) plots uses "live" IL: that means that something that in the "past" may have been an IL (in case of exit) now look as an impermanent gain (because price of asset has changed meanwhile)\
_This view seems more useful to me to people who enter and exit pool often, as the want to     consolidate their IG._
  </li>
  <li>
  PNL (%) plots uses NEG IL: that means, that pool changes dues to IL are always considered losses.\
  _This view seems more relevant to poeple who want to minimize risk, no matter market conditions (aka, no speculation on future value, no care to IG)_
  </li>
</ul>
