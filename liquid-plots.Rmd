---
title: "LiquidPlots"
output:
  html_document:
    df_print: paged
    code_folding: hide
---

Author: C0dingM0nk3y    [>>GitHub link<<](https://github.com/C0dingM0nk3y)

---

```{r init, include=FALSE}
#If not installed yet, install pacman package manager by "uncommenting" the line below
#install.packages("pacman")

### Dependences
#> API requests
pacman::p_load('digest') #https://www.rdocumentation.org/packages/digest/versions/0.6.29/topics/hmac
pacman::p_load('httr') #API tools #https://www.dataquest.io/blog/r-api-tutorial/
pacman::p_load('jsonlite')

#> Data Analysis 
pacman::p_load('magrittr')
pacman::p_load('stringr')
pacman::p_load("tidyverse")
#pacman::p_load("data.table")
pacman::p_load('kableExtra')
#pacman::p_load("lubridate")

#> Data visualization
#pacman::p_load("patchwork") #GUIDE: https://aosmith.rbind.io/2019/05/13/small-multiples-plot/

# REMOVE
#pacman::p_load("readxl")

#general options
options(digits = 12) #max decimal digits
options(scipen=999) #scientific notation threshold
```

```{r dirs and functions, include=FALSE}
# IMPORT: download of ORIGINAL data only 
dir.IN <- 'DOWNLOADS/' %T>% dir.create(showWarnings = F)
dir.IN.single <- paste0(dir.IN, "SinglePools/")  %T>% dir.create(showWarnings = F)

dir.TABLES <- 'TABLES/' %T>% dir.create(showWarnings = F)
dir.TABLES.single <-  paste0(dir.TABLES, "SinglePools/") %T>% dir.create(showWarnings = F)

dir.OUT <- 'OUTPUTS/' %T>% dir.create(showWarnings = F)

#for reference:
  "D:/Clouds/Dropbox/Everywhere/PROJECTS/PiggyBank/Binance/@SCRIPTS//Binance_FUNCTIONS_v1.9.R"

source("functions.R")
```

# {.tabset}

```{r code_folding="none"}
#RUN OPTIONS
run_api = TRUE #toggle TRUE/FALSE to skip some parts of this script
run_analysis = TRUE
run_plots = TRUE
```

---

## Part 1: DATA-DOWNLOAD

---

### API address and credentials
Import USER_KEY from *"/credentials.txt"*\
Use format and save as into [root dir] as *'credentials.txt'*:\
<p style="margin-left: 40px">
key;YOUR-BINANCE-API-KEY\
prKey;YOUR-PRIVATE-KEY\
</p>

```{r credentials, results = "asis"}
#API address
API_root <- "https://api.binance.com" 
  
if(run_api){
  myPrivatePath <- "D:/Clouds/Dropbox/Everywhere/PROJECTS/PiggyBank/Binance/liquid-plot_myCredential.txt" #TEMP
  
  #Import credentials
  #keys <- read.csv2("credentials.txt", col.names = c("Type", "Key"), header = F)
  keys_import <- read.csv2(myPrivatePath, col.names = c("Type", "Key"), header = F)
  
  key_public <- keys_import[1,"Key", drop=T]
  key_private <- keys_import[2,"Key", drop=T]
  rm("keys_import")
  
  #USER FEEDBACK
  if(key_public=="YOUR-BINANCE-API-KEY"){
    stop("STOP: add your Binance API credential to /credential.txt (; separated)")
  }
}
```

Data will be recovered from [`r API_root`] using the following credentials:\
<p style="margin-left: 40px">
**PUBLIC key**: `r ifelse(run_api, key_public, "SKIPPED")`\
**PRIVATE key**: [see "/credentials.txt"]
</p>

---

### DOWNLOAD Binance Data [API]

for info, refer to see: https://binance-docs.github.io/apidocs/spot/en/#change-log\
Download to: `r dir.IN.single`\
Download to: `r dir.IN`\

***
#### GET: PRICES
Latest SPOT prices from Binance

```{r price, results="hold"}
# DOWNLOAD PRICES
if(run_api){
  API_query <- "/api/v3/ticker/price"
  
  price.j.path <- paste0(dir.IN, "API_price.json")
  
  price.j <- BINANCE.GET(API_root, "/api/v3/ticker/price") #download Binance latest Prices
  
  write_json(price.j, price.j.path, auto_unbox=TRUE) #export
  cat("\t-> saved to: ", price.j.path, "\n")
  
  price.df <- fromJSON(price.j.path, simplifyVector = TRUE,
                     flatten = TRUE) #automatically UN-Nest nested columns
              
  write.csv2(price.df, file = str_replace(price.j.path, pattern = ".json", "_unpacked.csv"), row.names = F)
  cat(sprintf("\tCONVERTED TO .CSV\t-> export to: %s (%s rows)\n", 
              str_replace(price.j.path, pattern = ".json", "_unpacked.csv"), nrow(price.df)))
}else{cat("SKIPPED\n")}
```

***
#### GET: LIQUIDITY POSITIONS (Currently active)
Latest data about currently active pool and their Coin composition. 

```{r liquidity-now, results="hold"}
if(run_api){
  # DOWNLOAD
  liq.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/liquidity", timestamp = TRUE, sign = TRUE) 
  
  # JSON EXPORT
  liq.j.path <- paste0(dir.IN, "API_liq.json")
  write_json(liq.j, liq.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS
  cat(sprintf("\t-> export to: %s\n", liq.j.path))
  
  # TABLE INTERPRETATION
  #> this function does not ONLY unpack liquidity data, but it also unpivot the table, to make it readable
  liq.df <- liquidity.tableInterpreter(liq.j.path) 
  
  # TEMP WORKAROUND
          message("FITER APPLIED: only selected pools are kept")            #TEMP
          liq.df %<>% subset(grepl("BTC|ETH|SOL|DOGE|GMX", liq.df$poolName))  #TEMP
  
  liq.df.path <- paste0(dir.IN, "API_liquidity_unpacked.csv")
  write.csv2(liq.df, liq.df.path, row.names = F)
  cat(sprintf("\tUNPACKED INTO .CSV\t-> export to: %s (%s rows)\n", liq.df.path, nrow(liq.df)))
  
}else{cat("SKIPPED\n")}
```

#### PLOT: ACTIVE POOLS
found `r ifelse(run_api, length(poolNames_list), "X")` pools:

```{r active-pools}
if(run_api){
  
  activePools.df[,"Date_UTC"] <- msec_to_datetime(activePools.df$updateTime)
  
  #Print current Pools
  activePools.df %>% 
    subset(select = c("poolName", "poolId", "Date_UTC", "share.Amount")) %>% #select useful col.
    unique() %>% #filters out duplicated entries (2x for each pool)
    kable(align= "c", caption = "<b>Currently ACTIVE Pools</b>") %>%
    kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F) %>%
    column_spec(c(1), bold=T) 
  
}else{cat("SKIPPED\n")}
```

---

#### GET: HISTORICAL POOLDATA 
This includes Pool Operations (ADD/REMOVE) and Rewards Claims
_only data from the pools above is downloaded_


```{r singlePools, results="hold"}
# MAX trx x pool is 100. Use pagination if more is required (may be required for Claims)

poolId_list <- unique(activePools.df$poolId) #all unique poolId
poolNames_list <- unique(activePools.df$poolName) #unique poolNames

if(run_api){
  
  for (n in 1:length(poolId_list)){
    #POOL-NAME
    id <- poolId_list[n]
    pName <- poolNames_list[n]
    
    cat(sprintf("\n\tPool: %s (poolId=%s)\n", pName, id))
    
    #POOL OPERATIONS
    ops.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/liquidityOps", 
                       API_param = paste0("limit=100&poolId=",id),
                       timestamp = TRUE, sign = TRUE) 
    
    ops.j.path <- paste0(dir.IN.single, id, "_ops.json")
    write_json(ops.j, ops.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS/SinglePools/
    
    #CLAIMED DATA
    claim.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/claimedHistory", 
                       API_param = paste0("type=1&limit=100&poolId=",id), #type 0/1 = pending/successful
                       timestamp = TRUE, sign = TRUE) 
    
    claim.j.path <- paste0(dir.IN.single, id, "_claim.json")
    write_json(claim.j, claim.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS/SinglePools/
  }
  
}else{cat("SKIPPED\n")}
```


```{r cleanup1}
var_all <- ls()
var_keep <- c(grep(var_all, pattern = ".df|dir|DF|run", value=T),
              lsf.str()) # all functions names, also considered variables
var_rm <- setdiff(var_all,var_keep)              
rm(list=c(var_rm, "var_all", "var_keep", "var_rm"))
```


### Part 2: DATA ANALYSIS

```{r analysis-init}
# VARIABLES and DF that need to be re-imported before running Part 2
if(run_api==FALSE){
  price.df <- read.csv2(paste0(dir.IN,"API_price_unpacked.csv"))
  liq.df <- read.csv2(paste0(dir.IN,"API_liquidity_unpacked.csv"))
  }
  
# Useful lists
poolId_list <- unique(liq.df$poolId) #all unique poolId
poolNames_list <- unique(liq.df$poolName) #unique poolNames
  
# unique coins (to fetch prices)
coinList <- poolNames_list %>% str_split("/") %>% unlist() %>% 
  c("BNB") %>% #add BNB, as this will be needed for extraRewards calculations
  unique()
```

#### Price Matrix

```{r price-matrix, include=FALSE}
if(run_analysis){
  # EXTRACTS PRICES for the relevant coins
  priceMatrix <- data.frame()
  refCoin = "USDT"
  
  for (c in coinList){
    priceMatrix[c,refCoin] <- getPrice(price.df, c, refCoin = refCoin) %>% as.numeric()}

}else{cat("SKIPPED\n")}
```


#### Active Pools DATA

THE MODULE BELOW SHOULD GO TO ANALYTICS
probably also the one about prices
```{r active-calc}
if(run_analysis){
  # Assign price value to each coin
  liq.df[,"Price"] <- priceMatrix[liq.df$Coin, refCoin] #recover price from priceMatrix
  liq.df[,"Value"] <- with(liq.df, share.asset*Price)
  liq.df[,"Currency"] <- refCoin
  
  # RENAME and CALC Coin1/Coin2 var
  active.calc <- data.frame(
      poolId = liq.df$poolId,
      poolName = liq.df$poolName,
      Date_Unix = liq.df$updateTime,
      Date_UTC = liq.df$updateTime %>% msec_to_datetime(),
      share.Amount = liq.df$share.Amount %>% as.numeric(),
      Coin1 = str_split_i(liq.df$poolName,pattern = "/", 1), #split and return first coin
      Coin2 = str_split_i(liq.df$poolName,pattern = "/", 2), #split and return second coin
      Qnt = liq.df$share.asset,
      coinName = liq.df$Coin,
      Value = liq.df$Value,
      Currency = liq.df$Currency
      )
    
  # UNPIVOT table
  #1. assign "label" Coin1/Coin2
  active.calc[,"coinID"] <- ifelse(active.calc$coinName==active.calc$Coin1, "1", "2")
  
  #2. pivot wider
  active.calc2 <- pivot_wider(active.calc,
                              id_cols = c("Date_Unix", "poolId", "poolName", "share.Amount", "Coin1", "Coin2", "Currency"), #columns to keep 
                              names_from = "coinID",  names_sep = "", values_from = c("Qnt", "Value") )
  
  #3. Value TOT
  active.calc2[,"Value_TOT"] <-  with(active.calc2, Value1+Value2)
  
  # REODER and DROP intermediate calc. columns
  active.DF <- active.calc2[order(active.calc2$Value_TOT, decreasing = T),#order by value_TOT (HIGH)
                             c("poolId","poolName","Date_Unix","share.Amount", #re-order columns
                               "Coin1","Qnt1","Value1",
                               "Coin2","Qnt2","Value2",
                               "Value_TOT", "Currency")]
  
  write.csv2(active.DF, paste0(dir.TABLES, "ActivePools.csv"), row.names = F)
  
  
  #FANCY PLOT
  #Print current Pools
  active.DF %>% 
    subset(select = c("poolName", "poolId", "share.Amount", "Qnt1", "Coin1", "Qnt2", "Coin2", "Value_TOT", "Currency")) %>% #select useful col.
    kable(digits = c(2,2,2,4,4,4,4,2,2), align= c("c","c","r","r","l","r","l","r","l"), 
          caption = "<b>Currently ACTIVE Pools</b>") %>%
    kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F) %>%
    column_spec(c(4,5), color="darkblue") %>% 
    column_spec(c(1,8,9), bold=T) 
  

}else{cat("SKIPPED\n")}
```



```{r analysis-ops}
# Import JASON from "/DOWNLOAD/SinglePools/"
# Export data as unpivoted .csv into "/TABLES/SinglePools/"
# Finally, check if previous data are available and, if so, append new data (HISTORY)

if(run_analysis){
  
  for(id in poolId_list){
    # PATHS
    ops.j.path <- paste0(dir.IN.single,id,"_ops.json")
    ops.table.path <- paste0(dir.TABLES.single,id,"_ops.csv")
    claim.j.path <- paste0(dir.IN.single,id,"_claim.json")
    claim.table.path <- paste0(dir.TABLES.single,id,"_claim.csv")
    snap.table.path <- paste0(dir.TABLES.single,id,"_snapshots.csv")
    
    # INTEPRET
    ops_DF <- ops.tableInterpreter(ops.j.path)
    write.csv2(ops_DF, ops.table.path, row.names = F) # EXPORT
    
    # CLAIMED DATA
    claim_DF <- claim.tableInterpreter(claim.j.path)
    write.csv2(claim_DF, claim.table.path, row.names = F) # EXPORT
    
    # Last Pool Balance (SNAPSHOT)
    snap_DF <-  subset(active.DF, poolId==id)
    write.csv2(snap_DF, snap.table.path, row.names = F) # EXPORT
  }

}
```

>>>>> CONTINUE FROM HERE <<<<<
```{r analysis-join}
# This can be done as a RE-IMPORT, immediately before PLOTTING? To consider

#> Last operation MUST be ADD
#> other options are not supported yet
if (tail(ops_DF,1)[1,"operation"]=="REMOVE"){ 
  print(ops_DF)
  stop("Error: pools with multiple ADD/REMOVE operatio nare currently NOT supported (In progress)")}

# LAST OPS
ops.last <- ops_DF %>% tail(1)# required to recover Claimed data (ops.last.date)
ops.last.date <- ops.last[1,"Date_UTC", drop=T]

# Last CLAIM
claim.last <- subset(claim_df, Date_UTC > ops.last.date)

#calculate cumulData
claim.last <- replace_na(claim_DF, replace = list(claimed1=0, claimed2=0, claimed3=0)) #replace NA with 0
claim.last[,"Cumul1"] <- cumsum(claim_DF[,"claimed1"])
claim.last[,"Cumul2"] <- cumsum(claim_DF[,"claimed2"])
claim.last[,"Cumul3"] <- cumsum(claim_DF[,"claimed3"])

# Last DATA
liq.last <- subset(activePools.DF, poolId==id)

# MERGE TOGETHER (?)



ops.last
claim.last




```

```{r}
```

