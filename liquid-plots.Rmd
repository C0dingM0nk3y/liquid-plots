---
title: "LiquidPlots"
output:
  html_document:
    df_print: paged
    code_folding: hide
---

Author: C0dingM0nk3y    [>>GitHub link<<](https://github.com/C0dingM0nk3y)

---

```{r init, include=FALSE}
#If not installed yet, install pacman package manager by "uncommenting" the line below
#install.packages("pacman")

### Dependences
#> API requests
pacman::p_load('digest') #https://www.rdocumentation.org/packages/digest/versions/0.6.29/topics/hmac
pacman::p_load('httr') #API tools #https://www.dataquest.io/blog/r-api-tutorial/
pacman::p_load('jsonlite')

#> Data Analysis 
pacman::p_load('magrittr')
pacman::p_load('stringr')
pacman::p_load("tidyverse")
#pacman::p_load("data.table")
pacman::p_load('kableExtra')
#pacman::p_load("lubridate")

#> Data visualization
#pacman::p_load("patchwork") #GUIDE: https://aosmith.rbind.io/2019/05/13/small-multiples-plot/

# REMOVE
#pacman::p_load("readxl")

#general options
options(digits = 12) #max decimal digits
options(scipen=999) #scientific notation threshold
```

```{r dirs and functions, include=FALSE}
# IMPORT: download of ORIGINAL data only 
dir.IN <- 'DOWNLOADS/' %T>% dir.create(showWarnings = F)
dir.IN.single <- paste0(dir.IN, "SinglePools/")  %T>% dir.create(showWarnings = F)

dir.TABLES <- 'TABLES/' %T>% dir.create(showWarnings = F)
dir.TABLES.single <-  paste0(dir.TABLES, "SinglePools/") %T>% dir.create(showWarnings = F)

dir.REF <- 'REFDATA/' %T>% dir.create(showWarnings = F)
dir.REF.single <-  paste0(dir.REF, "SinglePools/") %T>% dir.create(showWarnings = F)

#for reference:
  "D:/Clouds/Dropbox/Everywhere/PROJECTS/PiggyBank/Binance/@SCRIPTS//Binance_FUNCTIONS_v1.9.R"

source("functions_data.R")
source("functions_plots.R")
```

# {.tabset}

```{r code_folding="none"}
#RUN OPTIONS
run_api = TRUE #toggle TRUE/FALSE to skip some parts of this script
run_analysis = TRUE
run_plots = TRUE
```

---

## Part 1: DATA-DOWNLOAD

---

### API address and credentials
Import USER_KEY from *"/credentials.txt"*\
Use format and save as into [root dir] as *'credentials.txt'*:\
<p style="margin-left: 40px">
key;YOUR-BINANCE-API-KEY\
prKey;YOUR-PRIVATE-KEY\
</p>

```{r credentials, results = "asis"}
#API address
API_root <- "https://api.binance.com" 
  
if(run_api){
  myPrivatePath <- "D:/Clouds/Dropbox/Everywhere/PROJECTS/PiggyBank/Binance/liquid-plot_myCredential.txt" #TEMP
  
  #Import credentials
  #keys <- read.csv2("credentials.txt", col.names = c("Type", "Key"), header = F)
  keys_import <- read.csv2(myPrivatePath, col.names = c("Type", "Key"), header = F)
  
  key_public <- keys_import[1,"Key", drop=T]
  key_private <- keys_import[2,"Key", drop=T]
  rm("keys_import")
  
  #USER FEEDBACK
  if(key_public=="YOUR-BINANCE-API-KEY"){
    stop("STOP: add your Binance API credential to /credential.txt (; separated)")
  }
}
```

Data will be recovered from [`r API_root`] using the following credentials:\
<p style="margin-left: 40px">
**PUBLIC key**: `r ifelse(run_api, key_public, "SKIPPED")`\
**PRIVATE key**: [see "/credentials.txt"]
</p>

---

### DOWNLOAD Binance Data [API]

for info, refer to see: https://binance-docs.github.io/apidocs/spot/en/#change-log\
Download to: `r dir.IN.single`\
Download to: `r dir.IN`\

***
#### GET: PRICES
Latest SPOT prices from Binance

```{r price, results="hold"}
# DOWNLOAD PRICES
if(run_api){
  API_query <- "/api/v3/ticker/price"
  
  price.j.path <- paste0(dir.IN, "API_price.json")
  
  price.j <- BINANCE.GET(API_root, "/api/v3/ticker/price") #download Binance latest Prices
  
  write_json(price.j, price.j.path, auto_unbox=TRUE) #export
  cat("\t-> saved to: ", price.j.path, "\n")
  
  price.df <- fromJSON(price.j.path, simplifyVector = TRUE,
                     flatten = TRUE) #automatically UN-Nest nested columns
  
  #append price datastamp (from file last edit)
  timestamp <- file.info(price.j.path)$mtime %>% as.POSIXct(tz="UTC") %>% 
    round(0) #this is to remove msec from time
  price.df[,"timestamp_UTC"] <- as.character(timestamp)
  
  write.csv2(price.df, file = str_replace(price.j.path, pattern = ".json", "_unpacked.csv"), row.names = F)
  cat(sprintf("\tCONVERTED TO .CSV\t-> export to: %s (%s rows)\n", 
              str_replace(price.j.path, pattern = ".json", "_unpacked.csv"), nrow(price.df)))
}else{cat("SKIPPED\n")}
```

***
#### GET: LIQUIDITY POSITIONS (Currently active)
Latest data about currently active pool and their Coin composition. 

```{r liquidity-now, results="hold"}
if(run_api){
  # DOWNLOAD
  liq.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/liquidity", timestamp = TRUE, sign = TRUE) 
  
  # JSON EXPORT
  liq.j.path <- paste0(dir.IN, "API_liq.json")
  write_json(liq.j, liq.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS
  cat(sprintf("\t-> export to: %s\n", liq.j.path))
  
  # TABLE INTERPRETATION
  #> this function does not ONLY unpack liquidity data, but it also unpivot the table, to make it readable
  liq.df <- liquidity.tableInterpreter(liq.j.path) 
  
  # TEMP WORKAROUND
          message("FITER APPLIED: only selected pools are kept")            #TEMP
          liq.df %<>% subset(grepl("BTC|ETH|SOL|DOGE|GMX", liq.df$poolName))  #TEMP
  
  liq.df.path <- paste0(dir.IN, "API_liquidity_unpacked.csv")
  write.csv2(liq.df, liq.df.path, row.names = F)
  cat(sprintf("\tUNPACKED INTO .CSV\t-> export to: %s (%s rows)\n", liq.df.path, nrow(liq.df)))
  
}else{cat("SKIPPED\n")}
```

#### PLOT: ACTIVE POOLS
found `r ifelse(run_api, length(poolNames_list), "X")` pools:

```{r active-pools}
if(run_api){
  
  liq.df[,"Date_UTC"] <- msec_to_datetime(liq.df$updateTime)
  
  #Print current Pools
  liq.df %>% 
    subset(select = c("poolName", "poolId", "Date_UTC", "share.Amount")) %>% #select useful col.
    unique() %>% #filters out duplicated entries (2x for each pool)
    kable(align= "c", caption = "<b>Currently ACTIVE Pools</b>") %>%
    kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F) %>%
    column_spec(c(1), bold=T) 
  
}else{cat("SKIPPED\n")}
```

---

#### GET: HISTORICAL POOLDATA 
This includes Pool Operations (ADD/REMOVE) and Rewards Claims
_only data from the pools above is downloaded_


```{r singlePools, results="hold"}
# MAX trx x pool is 100. Use pagination if more is required (may be required for Claims)

poolId_list <- unique(liq.df$poolId) #all unique poolId
poolNames_list <- unique(liq.df$poolName) #unique poolNames

if(run_api){
  
  for (n in 1:length(poolId_list)){
    #POOL-NAME
    id <- poolId_list[n]
    pName <- poolNames_list[n]
    
    cat(sprintf("\n\tPool: %s (poolId=%s)\n", pName, id))
    
    #POOL OPERATIONS
    ops.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/liquidityOps", 
                       API_param = paste0("limit=100&poolId=",id),
                       timestamp = TRUE, sign = TRUE) 
    
    ops.j.path <- paste0(dir.IN.single, id, "_ops.json")
    write_json(ops.j, ops.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS/SinglePools/
    
    #CLAIMED DATA
    claim.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/claimedHistory", 
                       API_param = paste0("type=1&limit=100&poolId=",id), #type 0/1 = pending/successful
                       timestamp = TRUE, sign = TRUE) 
    
    claim.j.path <- paste0(dir.IN.single, id, "_claim.json")
    write_json(claim.j, claim.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS/SinglePools/
  }
  
}else{cat("SKIPPED\n")}
```


```{r cleanup1}
var_all <- ls()
var_keep <- c(grep(var_all, pattern = ".df|dir|DF|run", value=T),
              lsf.str()) # all functions names, also considered variables
var_rm <- setdiff(var_all,var_keep)              
rm(list=c(var_rm, "var_all", "var_keep", "var_rm"))
```


## Part 2: DATA ANALYSIS

```{r analysis-init}
# VARIABLES and DF that need to be re-imported before running Part 2
if(run_api==FALSE){
  price.df <- read.csv2(paste0(dir.IN,"API_price_unpacked.csv"))
  liq.df <- read.csv2(paste0(dir.IN,"API_liquidity_unpacked.csv"))
  }
  
# Useful lists
poolId_list <- unique(liq.df$poolId) #all unique poolId
poolNames_list <- unique(liq.df$poolName) #unique poolNames
  
# unique coins (to fetch prices)
coinList <- poolNames_list %>% str_split("/") %>% unlist() %>% 
  c("BNB") %>% #add BNB, as this will be needed for extraRewards calculations
  unique()
```

### Price Matrix
Extract prices from latest API [`r price.df[1,"timestamp_UTC"]`]:\

```{r price-matrix, include=FALSE}
refCoin = "USDT"

if(run_analysis){
  # EXTRACTS PRICES for the relevant coins
  priceMatrix <- data.frame()
  
  for (c in coinList){
    priceMatrix[c,refCoin] <- getPrice(price.df, c, refCoin = refCoin) %>% as.numeric()}

}else{cat("SKIPPED\n")}
```

All prices are expressed in **`r refCoin`**

#### Active Pools DATA
Overview of latest pools [currently active]

>>>THIS BELO SHOULD GO INTO A FUNCTION<<<

```{r active-calc}
#> Unpivot active-pools
#> Append current prices
#> Calculate values
 
if(run_analysis){
  # Assign price value to each coin
  liq.df[,"Price"] <- priceMatrix[liq.df$Coin, refCoin] #recover price from priceMatrix
  liq.df[,"Value"] <- with(liq.df, share.asset*Price) %>% round(8)
  liq.df[,"Currency"] <- refCoin
  
  # perfrom all CALCULATION in separate FUNC
  active.DF <- activePools.Calc(liq.df)
  
  # EXPORT
  write.csv2(active.DF, paste0(dir.TABLES, "ActivePools.csv"), row.names = F)
  
  # USER PLOT
  # Print current Pools
  active.DF %>% subset(select = c("poolName", "poolId", "Date_UTC", 
                                  "Qnt1", "Coin1", "Qnt2", "Coin2", 
                                  "Value_TOT", "Currency")) %>% #select useful col.
    kable(digits = c(2,2,2,4,4,4,4,2,2), align= c("c","c","r","r","l","r","l","r","l"), 
          caption = "<b>Currently ACTIVE Pools</b>") %>%
    kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F) %>%
    column_spec(c(4,5), color="darkblue") %>% 
    column_spec(c(1,8,9), bold=T) 
  

}else{cat("SKIPPED\n")}
```

#### Single Pools DATA

Import JASON from "/DOWNLOAD/SinglePools/"\
Export data as unpivoted .csv into "/TABLES/SinglePools/"\
Finally, check if previous data are available and, if so, append new data (HISTORY)\

>>> Consider a wrapper <<<

```{r analysis-ops}
# Import JASON from "/DOWNLOAD/SinglePools/"
# Export data as unpivoted .csv into "/TABLES/SinglePools/"
# Finally, check if previous data are available and, if so, append new data (HISTORY)

if(run_analysis){
  
  for(id in poolId_list){
    
    cat(sprintf("Exporting (poolId=%s) to /TABLES/SinglePools/:", id))
    # PATHS
    ops.j.path <- paste0(dir.IN.single,id,"_ops.json")
    ops.table.path <- paste0(dir.TABLES.single,id,"_ops.csv")
    claim.j.path <- paste0(dir.IN.single,id,"_claim.json")
    claim.table.path <- paste0(dir.TABLES.single,id,"_claim.csv")
    #snap.table.path <- paste0(dir.TABLES.single,id,"_snapshot.csv")
    snap.history.path <- paste0(dir.TABLES.single,id,"_snapshots.History.csv")
    
    # INTEPRET
    ops_DF <- ops.tableInterpreter(ops.j.path)
    write.csv2(ops_DF, ops.table.path, row.names = F) # EXPORT
    cat(sprintf("\t _ops.csv (%s rows)", nrow(ops_DF)))
    
    # CLAIMED DATA
    claim_DF <- claim.tableInterpreter(claim.j.path)
    write.csv2(claim_DF, claim.table.path, row.names = F) # EXPORT
    cat(sprintf("\t _claim.csv (%s rows)", nrow(claim_DF)))
    
    # Last Pool Balance (SNAPSHOT)
    snap_DF <-  subset(active.DF, poolId==id)
    #write.csv2(snap_DF, snap.table.path, row.names = F) # EXPRT? ACTUALLY only HIST is needed
    #cat(sprintf("\t snapshots.csv (%s rows)", nrow(snap_DF))) #this feedback is useless because it is always 1 row
    
    # Update/Crete HISTORY file (Snapshots)
    if (file.exists(snap.history.path)){
      snap_prev <- read.csv2(snap.history.path)} #import prev history
    else{snap_prev <- snap_DF} #...or replace it with latest data
    
    snap_merge <- bind_rows(list(snap_prev, snap_DF)) #merge with latest data 
    snap_H <- snap_merge[(!duplicated(snap_merge$Date_Unix)),] #removes duplicates based on UNIX time
    snap_H <- snap_H[order(snap_H$Date_Unix), ] #order chronologically
    
    write.csv2(snap_H, snap.history.path, row.names = F) # EXPORT HIST file
    cat(sprintf("\t _snapshots.History.csv (read %s rows, write %s)\n", nrow(snap_prev), nrow(snap_H)))
  }
}
```

## Part 3: PLOTS

TO IMPLEMENT: replace imports with history files

Make function poolAnalaysis: 
Input: 3x paths
Output: 5x DF?? (general, claim, ss, end, start?)

Naaaa! Better
3x functions <- claimAnalysis(path, startDate) -> returns CALC
                [...]
                Then run Plot functions directly

```{r analysis-join}
# This can be done as a RE-IMPORT, immediately before PLOTTING? To consider

# Maybe a function, whose imput is poolID?

id = 33

# IMPORT snap_H/ops/claim
ops.table.path <- paste0(dir.TABLES.single,id,"_ops.csv")
claim.table.path <- paste0(dir.TABLES.single,id,"_claim.csv")
snap.history.path <- paste0(dir.TABLES.single,id,"_snapshots.History.csv")
refData.path <- paste0(dir.REF.single,id,"_refData.csv")

ops_H <- read.csv2(ops.table.path) #REPLACE WITH HIST FILE
ops_H[,"Date_UTC"] %<>% as.POSIXct(tz="UTC")

claim_H <- read.csv2(claim.table.path) #REPLACE WITH HIST FILE
claim_H[,"Date_UTC"] %<>% as.POSIXct(tz="UTC")

snap_H <- read.csv2(snap.history.path)
snap_H[,"Date_UTC"] %<>% as.POSIXct(tz="UTC")


# POOL INFO ####
poolName <- snap_H[1,"poolName", drop=T]
coin1 <- snap_H[1,"Coin1", drop=T]
coin2 <- snap_H[1,"Coin2", drop=T]
coin3 <- claim_H[1,"Coin3", drop=T] # coin 3 is recovered from claim_CALC

# CURRENT MARKET DATA ####
price1 <- priceMatrix[coin1,refCoin]
price2 <- priceMatrix[coin2,refCoin]
price3 <- priceMatrix[coin3,refCoin]

# STARTPOINT CALCULATIONS  ####
#> All calc related to current (final) state of Pool
#>> NOTE: in current implementation, it is expected for the LAST operation to be a ADD. 
#>> Future implementation will be able to interpret more complex (multi ADD/RMOVE) operation, 
#>> for now it is only plotting results from LAST operation
ops.last <- ops_H %>% tail(1)# required to recover Claimed data (ops.last.date)
#sanity check
if (ops.last[1,"operation"]=="REMOVE"){ 
  print(ops_H)
  stop("Error: pools with multiple ADD/REMOVE operations are currently NOT supported (In progress)")}

#extract data
start_date <- ops.last[1,"Date_UTC", drop=T]
start_qnt1 <- ops.last[1,"Qnt1", drop=T] %>% as.numeric()
start_qnt2 <- ops.last[1,"Qnt2", drop=T] %>% as.numeric()

start_value <- (start_qnt1*price1) + (start_qnt2*price2)

#this is the most important param, as it define the center point for the IL plots.
entryRatio <- start_qnt1/start_qnt2
entryPrice <- 1/entryRatio

# SNAPSHOTS-HISTORY (used to rebuild price section)  ####
snap_H[,"PoolRatio"] <- with(snap_H, Qnt1/Qnt2) 
snap_H[,"PoolPrice"] <- 1/snap_H$PoolRatio
snap_H[,"PriceChange"] <- with(snap_H, PoolPrice/entryPrice) 

# CLAIM CALCULATIONS ####
#> scan the full history of pool claims, and FILTER for those that happened AFTER 'poolStart' date

claim_CALC <- subset(claim_H, Date_UTC > start_date)

#calculate cumulData
claim_CALC %<>% replace_na(replace = list(claimed1=0, claimed2=0, claimed3=0)) #replace NA with 0
claim_CALC[,"Cum_Qnt1"] <- cumsum(claim_CALC[,"claimed1"])
claim_CALC[,"Cum_Qnt2"] <- cumsum(claim_CALC[,"claimed2"])
claim_CALC[,"Cum_Qnt3"] <- cumsum(claim_CALC[,"claimed3"])

#calculate Cumul%, expressed as %of new coin earned copared to start_qnt
#> this is only calculated on coin1/coin2)
claim_CALC[,"Cum_%1"] <- claim_CALC[,"Cum_Qnt1"]/start_qnt1
claim_CALC[,"Cum_%2"] <- claim_CALC[,"Cum_Qnt2"]/start_qnt2

#Converts to Value
claim_CALC[,"Cum_Val1"] <- claim_CALC[,"Cum_Qnt1"]*price1
claim_CALC[,"Cum_Val2"] <- claim_CALC[,"Cum_Qnt2"]*price2
claim_CALC[,"Cum_Val3"] <- claim_CALC[,"Cum_Qnt3"]*price3

#Total Earn: Value and %
claim_CALC[,"Cum_ValTOT"] <- with(claim_CALC, (Cum_Val1+Cum_Val2+Cum_Val3))
claim_CALC[,"Cum_%ValTOT"] <- with(claim_CALC, Cum_ValTOT/start_value)



# ENDPOINTS CALCULATIONS ####
#> All calc related to current (final) state of Pool

end_date <- max(claim_CALC$Date_UTC, snap_H$Date_UTC)

# [Optional] REFDATA ####

if (file.exists(refData.path)){
  pool_REF <- read.csv2(refData.path) %>% 
    subset(Date_UTC > start_date) #filter
  
  pool_REF[order(pool_REF$updateTime), ]
  pool_REF[,"Date_UTC"] %<>% as.POSIXct(tz = "UTC")
  
}else{pool_REF <- FALSE}

```

Plot INPUTS
none? take needed info from Global?


```{r plots}
plot_trends <- LiqPlots_Trends()

plot_trends
```

