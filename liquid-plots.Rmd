---
title: "LiquidPlots"
output:
  html_document:
    df_print: paged
    code_folding: hide
---

Author: C0dingM0nk3y    [>>GitHub link<<](https://github.com/C0dingM0nk3y)

---

# {.tabset}

[] Add descriptions\ 
[x] Change order of downloads\ 
[x] Improve coin filter\ 
[] Wrap ref data into zip file\
[x] Claim from start\
[] Chang plot var names so to erase previous data


```{r code_folding="none"}
#RUN OPTIONS
run_api = TRUE #toggle TRUE/FALSE to skip some parts of this script
  downloadSubset = TRUE
  useSubset = TRUE
  subset_list <- c("BTC") #can be used to restrict analysis to a specific sibset of poolNames/Coins. Use REGEX syntax.
run_analysis = TRUE
run_plots = TRUE

refCoin = "USDT" #reference coin to express prices

```


```{r init, include=FALSE}
#If not installed yet, install pacman package manager by "uncommenting" the line below

#install.packages("pacman") #this is only required on first run.

source("_SCRIPTS/init_externalLibs.R") 
#If not installed yet, pacman will download and install all required packages on the first run of the script.

source("_SCRIPTS/init_functions.R")
source("_SCRIPTS/init_plots.R")
source("_SCRIPTS/init_dirs.R")
```

---

## Authentication

---

### API address and credentials
Import USER_KEY from *"/credentials.txt"*\
Use format and save as into [root dir] as *'credentials.txt'*:\
<p style="margin-left: 40px">
key;YOUR-BINANCE-API-KEY\
prKey;YOUR-PRIVATE-KEY\
</p>

```{r credentials, results = "asis"}
#API address
API_root <- "https://api.binance.com" 
  
myPrivatePath <- "D:/Clouds/Dropbox/Everywhere/PROJECTS/PiggyBank/Binance/liquid-plot_myCredential.txt" #TEMP

#Import credentials
#keys <- read.csv2("credentials.txt", col.names = c("Type", "Key"), header = F)
keys_import <- read.csv2(myPrivatePath, col.names = c("Type", "Key"), header = F)

key_public <- keys_import[1,"Key", drop=T]
key_private <- keys_import[2,"Key", drop=T]
rm("keys_import")

#USER FEEDBACK
if(key_public=="YOUR-BINANCE-API-KEY"){
  stop("STOP: add your Binance API credential to /credential.txt (; separated)")}
```

Data will be recovered from [`r API_root`] using the following credentials:\
<p style="margin-left: 40px">
**PUBLIC key**: `r ifelse(run_api, key_public, "SKIPPED")`\
**PRIVATE key**: [see "/credentials.txt"]
</p>

---

## 1. ACTIVE POSITIONS

### DOWNLOAD Binance Data [API]
for info, refer to: https://binance-docs.github.io/apidocs/spot/en/#change-log\
Download to: `r dir.IN`\
Download to: `r dir.IN.single`\ 

***

#### GET: LIQUIDITY POSITIONS (Currently active)
Latest data about currently active pool and their Coin composition. 

```{r liquidity-now, results="hold"}
if(run_api){
  # DOWNLOAD
  liq.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/liquidity", timestamp = TRUE, sign = TRUE) 
  
  # JSON EXPORT
  liq.j.path <- paste0(dir.IN, "API_liq.json")
  write_json(liq.j, liq.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS
  cat(sprintf("\t-> export to: %s\n", liq.j.path))
  
  # TABLE INTERPRETATION
  #> this function does not ONLY unpack liquidity data, but it also unpivot the table, to make it readable
  liq.df <- liquidity.tableInterpreter(liq.j.path) 

  if(downloadSubset){
    message("FILTER on DOWNLOAD: only selected pools are downloaded")            #TEMP
    liq.df %<>% subset(grepl(subset_list, liq.df$poolName))}  #TEMP
 
  liq.df.path <- paste0(dir.IN, "API_liquidity_unpacked.csv")
  write.csv2(liq.df, liq.df.path, row.names = F)
  cat(sprintf("\tUNPACKED INTO .CSV\t-> export to: %s (%s rows)\n", liq.df.path, nrow(liq.df)))
  
}else{
  cat(sprintf("Running Offline. Last pool data were imported from file:\n\t%s\n", paste0(dir.IN,"API_liquidity_unpacked.csv")))
  
  liq.df <- read.csv2(paste0(dir.IN,"API_liquidity_unpacked.csv"))}

if(useSubset){
  message("FILTER on CALC: only selected pools are shown")  
  tot_pools <- nrow(liq.df)
  liq.df %<>% subset(grepl(subset_list, liq.df$poolName))  
  cat(sprintf("\tAvailable: %s\tDisplayed: %s", tot_pools, nrow(liq.df)))
  }  
```

***

#### PLOT: ACTIVE POOLS
found `r ifelse(run_api, length(unique(liq.df$poolName)), "X")` pools:

```{r plot-active}
liq.df[,"Date_UTC"] <- msec_to_datetime(liq.df$updateTime)

#Print current Pools
liq.df %>% 
  subset(select = c("poolName", "poolId", "Date_UTC", "share.Amount")) %>% #select useful col.
  unique() %>% #filters out duplicated entries (2x for each pool)
  kable(align= "c", caption = "<b>Currently ACTIVE Pools</b>") %>%
  kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F) %>%
  column_spec(c(1), bold=T) 

```

#### GET: PRICES
Latest SPOT prices from Binance. Used for HODL vs. LP calculation\

```{r price, results="hold"}
# unique coins (to fetch prices)
coinList <- unique(liq.df$poolName) %>% 
  str_split("/") %>% unlist() %>% 
  c("BNB") %>% #add BNB, as this will be needed for extraRewards calculations
  unique()

# DOWNLOAD PRICES
price.j.path <- paste0(dir.IN, "API_price.json")
price.csv.path <- paste0(dir.IN, "API_price_unpacked.csv")

if(run_api){
  API_query <- "/api/v3/ticker/price"
  
  price.j <- BINANCE.GET(API_root, "/api/v3/ticker/price", output = "both") #download Binance latest Prices
  
  request <- price.j[[1]] 
  json <- price.j[[2]]
  
  write_json(json, price.j.path, auto_unbox=TRUE) #export
  cat("\t-> saved to: ", price.j.path, "\n")
  
  price.df <- fromJSON(price.j.path, simplifyVector = TRUE,
                     flatten = TRUE) #automatically UN-Nest nested columns
  
  #append price datastamp (from file last edit)
  timestamp <- request$date %>% as.POSIXct(tz="UTC")
  price.df[,"timestamp_UTC"] <- as.character(timestamp)
  
  write.csv2(price.df, file = price.csv.path, row.names = F)
  cat(sprintf("\tCONVERTED TO .CSV\t-> export to: %s (%s rows)\n", price.csv.path, nrow(price.df)))
}else{
  price.df <- read.csv2(price.csv.path)
  cat("Running Offline. Last available data from file:\n")
  cat("\t", price.csv.path)
  }
```

Latest prices timestamp: **`r price.df[1,"timestamp", drop= TRUE]`**\ 
\
All prices are expressed in **`r refCoin`**

```{r price-matrix, include=FALSE}
# EXTRACTS PRICES for the relevant coins
priceMatrix <- data.frame()

for (c in coinList){
  priceMatrix[c,refCoin] <- getPrice(price.df, c, refCoin = refCoin) %>% as.numeric()}

priceMatrix[, "timestamp"] <- price.df[1,"timestamp_UTC"]
print(priceMatrix)
```


```{r active-calc}
#> Unpivot active-pools
#> Append current prices
#> Calculate values
#> 
#> # Assign price value to each coin
liq.df[,"Price"] <- priceMatrix[liq.df$Coin, refCoin] #recover price from priceMatrix
liq.df[,"Value"] <- with(liq.df, share.asset*Price) %>% round(8)
liq.df[,"Currency"] <- refCoin

# perfrom all CALCULATION in separate FUNC
active.DF <- activePools.Calc(liq.df)

# EXPORT
write.csv2(active.DF, paste0(dir.TABLES, "ActivePools.csv"), row.names = F)
```


## 2. DOWNLOADS {.tabset}

Plotting paramenters:

```{r single-options}
setStopLoss <- 0.005 #set desired stop loss for plots

allow_refData = FALSE
max_iter = 10 #so far, only applies to Claimed. operations does not requires that
```

Click on tab below to select the pool to visualize

```{r loop-api, results='asis', fig.align='center', fig.height=6, fig.width=10}
# Useful lists
poolId_list <- unique(liq.df$poolId) #all unique poolId
poolNames_list <- unique(liq.df$poolName) #unique poolNames
  
for (x in 1:length(poolId_list)){
  #POOL-NAME
  id <- poolId_list[x]
  pName <- poolNames_list[x]
  
  cat(sprintf("### %s \n", pName)) # Tabs header
  
    cat("<div> \n") #needed to encapsulate plot element so not to distrupt the TAB function
      cat("<p>some parag. text</p>") # Tabs header
      
      # PATHS
      ops.j.path <- paste0(dir.IN.single,id,"_ops.json")
      ops.table.path <- paste0(dir.TABLES.single,id,"_ops.csv") #REMOVE after making sure there is conflict
      claim.j.path <- paste0(dir.IN.single,id,"_claim.json")
      claim.table.path <- paste0(dir.TABLES.single,id,"_claim.csv")
      pool.history.path <- paste0(dir.TABLES.single,id,"_pool.History.csv")
      
      # DOWNLOADS
        cat("#### GET: Pool Operations (ADD/REMOVE)\ \n")
        cat("<div> > \n") #needed to encapsulate plot element so not to distrupt the TAB function
          <<API-ops>>
        cat("</div>\n")
          
        cat("#### GET: Claimed Rewards\ \n")
        cat("<div> > \n") #needed to encapsulate plot element so not to distrupt the TAB function
          <<API-claim>>
        cat("</div>\n")
    cat("</div>\n")
  }

```

#####> GET OPS
```{r API-ops, results="hold", eval=FALSE}
# MAX trx x pool is 100. Use pagination if more is required (may be required for Claims)

# For each pool that is currently active:
# - Import JASON from "/DOWNLOAD/SinglePools/"\
# - Export data as _unpivoted.csv into "/TABLES/SinglePools/"\
# - Finally, check if previous data are available and, if so, append new data (HISTORY)\

ops.j.path <- paste0(dir.IN.single, id, "_ops.json")

if(run_api){
  #POOL OPERATIONS
  ops.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/liquidityOps", 
                     API_param = paste0("limit=100&poolId=",id),
                     timestamp = TRUE, sign = TRUE) 
  
  write_json(ops.j, ops.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS/SinglePools/
    
}else{
  ops.j <- read.csv2(ops.j.path)
  cat("Running Offline. Last available data from file:\n")
  cat("\t", ops.j.path)
  }

cat(sprintf("Exporting (poolId=%s) to /TABLES/SinglePools/:", id))
 
# INTEPRET ops table
ops_DF <- ops.tableInterpreter(ops.j.path)
write.csv2(ops_DF, ops.table.path, row.names = F) # EXPORT #REMOVE?
cat(sprintf("\t _ops.csv (%s rows)", nrow(ops_DF))) #REMOVE?

# Last Pool Balance (SNAPSHOT)
# Snapshot and OPS have similar format, so they are merged together and exported as poolHistory.csv
    
snap_DF <-  subset(active.DF, poolId==id)
#add missing columns before joining to ops table
snap_DF[,"shareAmount"] <- snap_DF[,"share.Amount"] #name change
snap_DF[,"operation"] <- "SNAPSHOT"

# Update/Crete HISTORY file (HISTORY)
if (file.exists(pool.history.path)){
  pool_prev <- read.csv2(pool.history.path) #import prev history
  #recalculate from Unix, that is always the safest option 
  #> reason: if the file was opened with Excel, it changes all data formats)
  pool_prev[, "Date_UTC"] <- msec_to_datetime(pool_prev$Date_Unix, tz = "UTC")
  }else{pool_prev <- snap_DF} #...or replace it with latest data

# JOIN and removes duplicates
common_cols <- intersect(colnames(ops_DF), colnames(pool_prev)) #why? because snap_DF has some extra cols with Values, and I want to drop them
pool_HIST <- bind_rows(list(ops_DF, #operations (OLD and NEW) 
                            pool_prev[,common_cols], # OLD DATA (including both OPS and SNAPS)
                            snap_DF[,common_cols])) # LAST SNAPSHOT (always NEW)
pool_HIST %<>% unique() #removes duplicates
pool_HIST <- pool_HIST[order(pool_HIST$Date_Unix), ] #order chronologically

write.csv2(pool_HIST, pool.history.path, row.names = F) # EXPORT HIST file
cat(sprintf("\t _pool.History.csv (read:%s, write:%s rows)\n", nrow(pool_prev), nrow(pool_HIST)))

```

#####> GET: Claim

```{r API-claim, eval=FALSE}
# start dowloading data from pool start (see ops module)
startTime <- ops_DF %>% 
  subset(operation=="ADD", select = Date_Unix, drop = T) %>% 
  tail(1)# find last ADD

claim.j_list <- list() #for data collection

# first ITER
claim.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/claimedHistory", 
                   API_param = sprintf("type=1&limit=100&startTime=%s&poolId=%s", startTime, id), #type 0/1 = pending/successful
                   timestamp = TRUE, sign = TRUE) 
claim.j_list[1] <- list(claim.j) #claim.j is already a list (1:100). This step creates a list of list.

# more ITER, if required
for (n in 1:max_iter){
  if (max_iter==0){break} #allow to break loop in case this is desirable (e.g. quick update)
  
  last_claim <- claim.j_list[[n]]
  if (length(last_claim)==100){ #that means that data reached limit (100) and need a separate API call to prev. one
    #output has earlier dates on top: first entry == higher date. 
    latestTimestamp <- last_claim[[1]]$claimedTime  #First entry [[1]] from last list [[n-1]]
    
    cat(paste0("\titer=",n+1,"\t")) #add an indentation and iter number to export      
    claim.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/claimedHistory", timestamp = TRUE, sign = TRUE,
                     API_param = paste0("type=1&limit=100&poolId=",id, "&startTime=", latestTimestamp)) 
                #Why lastTimestamp and NOT lastTimestamp-1
                # > each time stamps has 2-3 rows (one for each coin + extra rewards)
                # > there is 2/3 chance that last row of table is incomplete, and miss at least one of the coin. 
                # > therefore, it is better to download that again, and the remove duplicate entries in later steps
    
    claim.j_list[n+1] <- list(claim.j) #list of list (JSON is 1-100 list)
  }
  else{break} #break loop: last claim had less than 100 entries
}

# List JOIN and EXPORT (JASON)
claim.joined <- unlist(claim.j_list, recursive = F)

claim.j.path <- paste0(dir.IN.single, id, "_claim.json")
write_json(claim.joined, claim.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS/SinglePools/

if (length(claim.joined)>100){cat(sprintf("\t\t joined %s entries into single file: %s\n", length(claim.joined), claim.j.path))}

# Unpacking and EXPORT (CSV)
claim_DF <- claim.tableInterpreter(claim.j.path)
write.csv2(claim_DF, claim.table.path, row.names = F) # EXPORT
cat(sprintf("\t _claim.csv (%s rows)", nrow(claim_DF)))

```





#### Database Update

```{r}
#to be implemented
```

## 3. SINGLE POOLS {.tabset}
```{r loop-plots, results='asis', fig.align='center', fig.height=6, fig.width=10}
# Useful lists
poolId_list <- unique(liq.df$poolId) #all unique poolId
poolNames_list <- unique(liq.df$poolName) #unique poolNames
  
for (x in 1:length(poolId_list)){
  #POOL-NAME
  id <- poolId_list[x]
  pName <- poolNames_list[x]
  
  cat(sprintf("### %s{.tabset}\n", pName)) # Tabs header
  
    cat("<div> \n") #needed to encapsulate plot element so not to distrupt the TAB function
    cat(sprintf("<p>Pool: %s (poolId=%s)</p>\n", pName, id))
    
    # PLOTS
    source("_SCRIPTS/script_CALC_v0.R") # all CALC
    source("_SCRIPTS/script_PLOT_v0.R") # all PLOTS
    
    cat("#### PLOTS\ \n")
      cat("<div>\n") #needed to encapsulate plot element so not to distrupt the TAB function
      print(pw) #main figure
      cat("</div>\n")
  
    # Link to Binance pool
   cat(sprintf('<p>Direct Link to Binance Pool: <a href="https://www.binance.com/en/swap/liquidity?poolId=%s">(%s)</a></p>\n', id, poolName))
    
    cat("#### TABLES\ \n") # Tabs header
    cat("##### Pool Operations\n") # Tabs header
    data.table::data.table(pool_LAST) %>% kable() %>% print()
    cat("##### Pool Claims\n") 
    data.table::data.table(claim_CALC) %>% kable() %>% print()
    cat("##### Pool Endpoints\n")
    data.table::data.table(end_DF) %>% kable() %>% print()
    cat("</div> \n") 
  }
```


***

## 4. POOL OVERVIEW

TO IMPLEMENT\
[x] automated plotting\
[x] link to Binance pool\
[ ] Summary table (Analysis part)\
[ ] Subgroups\
                

Overview of latest pools [currently active]

```{r active-print}

 
# USER PLOT
# Print current Pools
active.DF %>% subset(select = c("poolName", "poolId", "Date_UTC", 
                                "Qnt1", "Coin1", "Qnt2", "Coin2", 
                                "Value_TOT", "Currency")) %>% #select useful col.
  kable(digits = c(2,2,2,4,4,4,4,2,2), align= c("c","c","r","r","l","r","l","r","l"), 
        caption = "<b>Currently ACTIVE Pools</b>") %>%
  kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F) %>%
  column_spec(c(4,5), color="darkblue") %>% 
  column_spec(c(1,8,9), bold=T) 
```


```{r overview, include=FALSE, eval=FALSE}
poolId_list_original <- poolId_list

summary.DF <- data.frame(active.DF, row.names = active.DF$poolId) 

for (i in poolId_list_original){
  pool.history.path <- paste0(dir.TABLES.single,i,"_pool.History.csv") #includes OPS and SNAPS
  #claim.table.path <- paste0(dir.TABLES.single,i,"_claim.csv")
  
  pool_H <- read.csv2(pool.history.path) #%>% as.tibble()
  pool_H[,"Date_UTC"] %<>% as.POSIXct(tz="UTC")
  #claim_H <- read.csv2(claim.table.path) #REPLACE WITH HIST FILE
  #claim_H[,"Date_UTC"] %<>% as.POSIXct(tz="UTC")

  start_DF <- pool_H %>% 
    subset(operation=="ADD") %>% 
    tail(1)# find last ADD
  
  summary.DF[as.character(i), "startDate"] <- start_DF[1, "Date_UTC", drop=T]
}

summary.DF <- summary.DF[order(summary.DF$Value_TOT, decreasing = T), ]
```

