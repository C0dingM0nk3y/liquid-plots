kable(align= "c", caption = "<b>Currently ACTIVE Pools</b>") %>%
kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F) %>%
column_spec(c(1), bold=T)
# unique coins (to fetch prices)
coinList <- unique(liq.df$poolName) %>%
str_split("/") %>% unlist() %>%
c("BNB") %>% #add BNB, as this will be needed for extraRewards calculations
unique()
# DOWNLOAD PRICES
price.j.path <- paste0(dir.IN, "API_price.json")
price.csv.path <- paste0(dir.IN, "API_price_unpacked.csv")
if(run_api){
API_query <- "/api/v3/ticker/price"
price.j <- BINANCE.GET(API_root, "/api/v3/ticker/price", output = "both") #download Binance latest Prices
request <- price.j[[1]]
json <- price.j[[2]]
write_json(json, price.j.path, auto_unbox=TRUE) #export
cat("\t-> saved to: ", price.j.path, "\n")
price.df <- fromJSON(price.j.path, simplifyVector = TRUE,
flatten = TRUE) #automatically UN-Nest nested columns
#append price datastamp (from file last edit)
timestamp <- request$date %>% as.POSIXct(tz="UTC")
price.df[,"timestamp_UTC"] <- as.character(timestamp)
write.csv2(price.df, file = price.csv.path, row.names = F)
cat(sprintf("\tCONVERTED TO .CSV\t-> export to: %s (%s rows)\n", price.csv.path, nrow(price.df)))
}else{
price.df <- read.csv2(price.csv.path)
cat("Running Offline. Last available data from file:\n")
cat("\t", price.csv.path)
}
# EXTRACTS PRICES for the relevant coins
priceMatrix <- data.frame()
for (c in coinList){
priceMatrix[c,refCoin] <- getPrice(price.df, c, refCoin = refCoin) %>% as.numeric()}
print(priceMatrix)
priceMatrix
# EXTRACTS PRICES for the relevant coins
priceMatrix <- data.frame()
for (c in coinList){
priceMatrix[c,refCoin] <- getPrice(price.df, c, refCoin = refCoin) %>% as.numeric()}
print(priceMatrix)
print(priceMatrix)
priceMatrix[, "timestamp"] <- price.df[1,"timestamp_UTC"]
priceMatrix
setStopLoss <- 0.005 #set desired stop loss for plots
allow_refData = FALSE
max_iter = 10 #so far, only applies to Claimed. operations does not requires that
#POOL-NAME
id <- poolId_list[x]
pName <- poolNames_list[x]
cat(sprintf("### %s <br>\n", pName)) # Tabs header
# MAX trx x pool is 100. Use pagination if more is required (may be required for Claims)
# For each pool that is currently active:
# - Import JASON from "/DOWNLOAD/SinglePools/"\
# - Export data as _unpivoted.csv into "/TABLES/SinglePools/"\
# - Finally, check if previous data are available and, if so, append new data (HISTORY)\
ops.j.path <- paste0(dir.IN.single, id, "_ops.json")
if(run_api){
#POOL OPERATIONS
ops.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/liquidityOps",
API_param = paste0("limit=100&poolId=",id),
timestamp = TRUE, sign = TRUE)
write_json(ops.j, ops.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS/SinglePools/
}else{
ops.j <- read.csv2(ops.j.path)
cat("Running Offline. Last available data from file:\n")
cat("\t", ops.j.path)
}
cat(sprintf("Exporting (poolId=%s) to /TABLES/SinglePools/:", id))
# INTEPRET ops table
ops_DF <- ops.tableInterpreter(ops.j.path)
write.csv2(ops_DF, ops.table.path, row.names = F) # EXPORT #REMOVE?
cat(sprintf("\t _ops.csv (%s rows)", nrow(ops_DF))) #REMOVE?
# Last Pool Balance (SNAPSHOT)
# Snapshot and OPS have similar format, so they are merged together and exported as poolHistory.csv
snap_DF <-  subset(active.DF, poolId==id)
#add missing columns before joining to ops table
snap_DF[,"shareAmount"] <- snap_DF[,"share.Amount"] #name change
snap_DF[,"operation"] <- "SNAPSHOT"
# Update/Crete HISTORY file (HISTORY)
if (file.exists(pool.history.path)){
pool_prev <- read.csv2(pool.history.path) #import prev history
#recalculate from Unix, that is always the safest option
#> reason: if the file was opened with Excel, it changes all data formats)
pool_prev[, "Date_UTC"] <- msec_to_datetime(pool_prev$Date_Unix, tz = "UTC")}
else{pool_prev <- snap_DF} #...or replace it with latest data
# MAX trx x pool is 100. Use pagination if more is required (may be required for Claims)
# For each pool that is currently active:
# - Import JASON from "/DOWNLOAD/SinglePools/"\
# - Export data as _unpivoted.csv into "/TABLES/SinglePools/"\
# - Finally, check if previous data are available and, if so, append new data (HISTORY)\
ops.j.path <- paste0(dir.IN.single, id, "_ops.json")
if(run_api){
#POOL OPERATIONS
ops.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/liquidityOps",
API_param = paste0("limit=100&poolId=",id),
timestamp = TRUE, sign = TRUE)
write_json(ops.j, ops.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS/SinglePools/
}else{
ops.j <- read.csv2(ops.j.path)
cat("Running Offline. Last available data from file:\n")
cat("\t", ops.j.path)
}
cat(sprintf("Exporting (poolId=%s) to /TABLES/SinglePools/:", id))
# INTEPRET ops table
ops_DF <- ops.tableInterpreter(ops.j.path)
write.csv2(ops_DF, ops.table.path, row.names = F) # EXPORT #REMOVE?
cat(sprintf("\t _ops.csv (%s rows)", nrow(ops_DF))) #REMOVE?
# Last Pool Balance (SNAPSHOT)
# Snapshot and OPS have similar format, so they are merged together and exported as poolHistory.csv
snap_DF <-  subset(active.DF, poolId==id)
#add missing columns before joining to ops table
snap_DF[,"shareAmount"] <- snap_DF[,"share.Amount"] #name change
snap_DF[,"operation"] <- "SNAPSHOT"
# Update/Crete HISTORY file (HISTORY)
if (file.exists(pool.history.path)){
pool_prev <- read.csv2(pool.history.path) #import prev history
#recalculate from Unix, that is always the safest option
#> reason: if the file was opened with Excel, it changes all data formats)
pool_prev[, "Date_UTC"] <- msec_to_datetime(pool_prev$Date_Unix, tz = "UTC")
}else{pool_prev <- snap_DF} #...or replace it with latest data
# JOIN and removes duplicates
common_cols <- intersect(colnames(ops_DF), colnames(pool_prev)) #why? because snap_DF has some extra cols with Values, and I want to drop them
pool_HIST <- bind_rows(list(ops_DF, #operations (OLD and NEW)
pool_prev[,common_cols], # OLD DATA (including both OPS and SNAPS)
snap_DF[,common_cols])) # LAST SNAPSHOT (always NEW)
pool_HIST %<>% unique() #removes duplicates
pool_HIST <- pool_HIST[order(pool_HIST$Date_Unix), ] #order chronologically
write.csv2(pool_HIST, pool.history.path, row.names = F) # EXPORT HIST file
cat(sprintf("\t _pool.History.csv (read:%s, write:%s rows)\n", nrow(pool_prev), nrow(pool_HIST)))
# JOIN and removes duplicates
common_cols <- intersect(colnames(ops_DF), colnames(pool_prev)) #why? because snap_DF has some extra cols with Values, and I want to drop them
pool_HIST <- bind_rows(list(ops_DF, #operations (OLD and NEW)
pool_prev[,common_cols], # OLD DATA (including both OPS and SNAPS)
snap_DF[,common_cols])) # LAST SNAPSHOT (always NEW)
pool_HIST %<>% unique() #removes duplicates
pool_HIST <- pool_HIST[order(pool_HIST$Date_Unix), ] #order chronologically
write.csv2(pool_HIST, pool.history.path, row.names = F) # EXPORT HIST file
cat(sprintf("\t _pool.History.csv (read:%s, write:%s rows)\n", nrow(pool_prev), nrow(pool_HIST)))
# start dowloading data from pool start (see ops module)
startTime <- ops_DF %>%
subset(operation=="ADD", select = Date_Unix, drop = T) %>%
tail(1)# find last ADD
claim.j_list <- list() #for data collection
# first ITER
claim.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/claimedHistory",
API_param = sprintf("type=1&limit=100&startTime=%s&poolId=%s", startTime, id), #type 0/1 = pending/successful
timestamp = TRUE, sign = TRUE)
claim.j_list[1] <- list(claim.j) #claim.j is already a list (1:100). This step creates a list of list.
# more ITER, if required
for (n in 1:max_iter){
if (max_iter==0){break} #allow to break loop in case this is desirable (e.g. quick update)
last_claim <- claim.j_list[[n]]
if (length(last_claim)==100){ #that means that data reached limit (100) and need a separate API call to prev. one
#output has earlier dates on top: first entry == higher date.
latestTimestamp <- last_claim[[1]]$claimedTime  #First entry [[1]] from last list [[n-1]]
cat(paste0("\titer=",n+1,"\t")) #add an indentation and iter number to export
claim.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/claimedHistory", timestamp = TRUE, sign = TRUE,
API_param = paste0("type=1&limit=100&poolId=",id, "&startTime=", latestTimestamp))
#Why lastTimestamp and NOT lastTimestamp-1
# > each time stamps has 2-3 rows (one for each coin + extra rewards)
# > there is 2/3 chance that last row of table is incomplete, and miss at least one of the coin.
# > therefore, it is better to download that again, and the remove duplicate entries in later steps
claim.j_list[n+1] <- list(claim.j) #list of list (JSON is 1-100 list)
}
else{break} #break loop: last claim had less than 100 entries
}
# List JOIN and EXPORT (JASON)
claim.joined <- unlist(claim.j_list, recursive = F)
claim.j.path <- paste0(dir.IN.single, id, "_claim.json")
write_json(claim.joined, claim.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS/SinglePools/
if (length(claim.joined)>100){cat(sprintf("\t\t joined %s entries into single file: %s\n", length(claim.joined), claim.j.path))}
# Unpacking and EXPORT (CSV)
claim_DF <- claim.tableInterpreter(claim.j.path)
write.csv2(claim_DF, claim.table.path, row.names = F) # EXPORT
cat(sprintf("\t _claim.csv (%s rows)", nrow(claim_DF)))
ops_DF
active.DF
active.DF
active.DF
active.DF
active.DF
#> Unpivot active-pools
#> Append current prices
#> Calculate values
#>
#> # Assign price value to each coin
liq.df[,"Price"] <- priceMatrix[liq.df$Coin, refCoin] #recover price from priceMatrix
liq.df[,"Value"] <- with(liq.df, share.asset*Price) %>% round(8)
liq.df[,"Currency"] <- refCoin
# perfrom all CALCULATION in separate FUNC
active.DF <- activePools.Calc(liq.df)
# EXPORT
write.csv2(active.DF, paste0(dir.TABLES, "ActivePools.csv"), row.names = F)
cat("<p>some parag.r. text</p>") # Tabs header
# PLOTS
source("_SCRIPTS/script_CALC_v0.R") # all CALC
source("_SCRIPTS/script_PLOT_v0.R") # all PLOTS
cat(sprintf("### %s{.tabset}\n", pName)) # Tabs header
# Link to Binance pool
cat(sprintf('<p>Direct Link to Binance Pool: <a href="https://www.binance.com/en/swap/liquidity?poolId=%s">(%s)</a></p>\n', id, poolName))
#RUN OPTIONS
run_api = TRUE #toggle TRUE/FALSE to skip some parts of this script
downloadSubset = TRUE
useSubset = TRUE
subset_list <- c("BTC") #can be used to restrict analysis to a specific sibset of poolNames/Coins. Use REGEX syntax.
run_analysis = TRUE
run_plots = TRUE
refCoin = "USDT" #reference coin to express prices
# unique coins (to fetch prices)
coinList <- unique(liq.df$poolName) %>%
str_split("/") %>% unlist() %>%
c("BNB") %>% #add BNB, as this will be needed for extraRewards calculations
unique()
# DOWNLOAD PRICES
price.j.path <- paste0(dir.IN, "API_price.json")
price.csv.path <- paste0(dir.IN, "API_price_unpacked.csv")
if(run_api){
API_query <- "/api/v3/ticker/price"
price.j <- BINANCE.GET(API_root, "/api/v3/ticker/price", output = "both") #download Binance latest Prices
request <- price.j[[1]]
json <- price.j[[2]]
write_json(json, price.j.path, auto_unbox=TRUE) #export
cat("\t-> saved to: ", price.j.path, "\n")
price.df <- fromJSON(price.j.path, simplifyVector = TRUE,
flatten = TRUE) #automatically UN-Nest nested columns
#append price datastamp (from file last edit)
timestamp <- request$date %>% as.POSIXct(tz="UTC")
price.df[,"timestamp_UTC"] <- as.character(timestamp)
write.csv2(price.df, file = price.csv.path, row.names = F)
cat(sprintf("\tCONVERTED TO .CSV\t-> export to: %s (%s rows)\n", price.csv.path, nrow(price.df)))
}else{
price.df <- read.csv2(price.csv.path)
cat("Running Offline. Last available data from file:\n")
cat("\t", price.csv.path)
}
#> Unpivot active-pools
#> Append current prices
#> Calculate values
#>
#> # Assign price value to each coin
liq.df[,"Price"] <- priceMatrix[liq.df$Coin, refCoin] #recover price from priceMatrix
liq.df[,"Value"] <- with(liq.df, share.asset*Price) %>% round(8)
liq.df[,"Currency"] <- refCoin
# perfrom all CALCULATION in separate FUNC
active.DF <- activePools.Calc(liq.df)
# EXPORT
write.csv2(active.DF, paste0(dir.TABLES, "ActivePools.csv"), row.names = F)
# EXTRACTS PRICES for the relevant coins
priceMatrix <- data.frame()
for (c in coinList){
priceMatrix[c,refCoin] <- getPrice(price.df, c, refCoin = refCoin) %>% as.numeric()}
priceMatrix[, "timestamp"] <- price.df[1,"timestamp_UTC"]
rmarkdown::paged_table(priceMatrix)
# EXTRACTS PRICES for the relevant coins
priceMatrix <- data.frame()
for (c in coinList){
priceMatrix[c,refCoin] <- getPrice(price.df, c, refCoin = refCoin) %>% as.numeric()}
priceMatrix[, "timestamp"] <- price.df[1,"timestamp_UTC"]
rmarkdown::paged_table(priceMatrix)
ops.table.path
View(ops_DF)
active.DF
#Print current Pools
active.DF %>%
subset(select = c("poolName", "poolId", "Date_UTC", "share.Amount", "Value", "Currency")) %>% #select useful col.
unique() %>% #filters out duplicated entries (2x for each pool)
kable(align= "c", caption = "<b>Currently ACTIVE Pools</b>") %>%
kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F) %>%
column_spec(c(1), bold=T)
#Print current Pools
active.DF %>%
subset(select = c("poolName", "poolId", "Date_UTC", "share.Amount", "Value_TOT", "Currency")) %>% #select useful col.
unique() %>% #filters out duplicated entries (2x for each pool)
kable(align= "c", caption = "<b>Currently ACTIVE Pools</b>") %>%
kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F) %>%
column_spec(c(1), bold=T)
#Print current Pools
active.DF %>%
subset(select = c("poolName", "poolId", "Date_UTC", "share.Amount", "Value_TOT", "Currency")) %>% #select useful col.
unique() %>% #filters out duplicated entries (2x for each pool)
kable(align= "c", digits = 2, caption = "<b>Currently ACTIVE Pools</b>") %>%
kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F) %>%
column_spec(c(1), bold=T)
liq.df
# FORMAT to PRINT
active.table <- liq.df %>%
subset(select = c("poolName", "poolId", "share.Amount", "Date_UTC")) %>% #select useful col.
unique() %>% #filters out duplicated entries (2x for each pool)
kable(align= "c", digits = 2, caption = "<b>Currently ACTIVE Pools</b>") %>%
kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F) %>%
column_spec(c(1), bold=T)
active.table
# FORMAT to PRINT
active.table <- liq.df %>%
subset(select = c("poolName", "poolId", "share.Amount", "Date_UTC")) %>% #select useful col.
unique() %>% #filters out duplicated entries (2x for each pool)
kable(align= "c", digits = 2, caption = "<b>Currently ACTIVE Pools</b>") %>%
kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F) %>%
column_spec(c(1), bold=T)
cat(sprintf("Exporting (poolId=%s) to /TABLES/SinglePools/:", id))
setwd("~/GitHub/liquid-plots")
# Useful lists
poolId_list <- unique(liq.df$poolId) #all unique poolId
poolNames_list <- unique(liq.df$poolName) #unique poolNames
if(run_api){
for (x in 1:length(poolId_list)){
#POOL-NAME
id <- poolId_list[x]
pName <- poolNames_list[x]
cat(sprintf("#### %s \n", pName)) # Tabs header
source("_SCRIPTS/script_SINGLE.R")
}
else{
# Useful lists
poolId_list <- unique(liq.df$poolId) #all unique poolId
poolNames_list <- unique(liq.df$poolName) #unique poolNames
if(run_api){
for (x in 1:length(poolId_list)){
#POOL-NAME
id <- poolId_list[x]
pName <- poolNames_list[x]
cat(sprintf("#### %s \n", pName)) # Tabs header
source("_SCRIPTS/script_SINGLE.R")
}else{
# Useful lists
poolId_list <- unique(liq.df$poolId) #all unique poolId
poolNames_list <- unique(liq.df$poolName) #unique poolNames
if(run_api){
for (x in 1:length(poolId_list)){
#POOL-NAME
id <- poolId_list[x]
pName <- poolNames_list[x]
cat(sprintf("#### %s \n", pName)) # Tabs header
source("_SCRIPTS/script_SINGLE.R")}
}else{
ops.j <- read.csv2(ops.j.path)
cat("Running Offline. Last available data from file:\n")
cat("\t", ops.j.path)
}
}
# Useful lists
poolId_list <- unique(liq.df$poolId) #all unique poolId
poolNames_list <- unique(liq.df$poolName) #unique poolNames
if(run_api){
for (x in 1:length(poolId_list)){
#POOL-NAME
id <- poolId_list[x]
pName <- poolNames_list[x]
cat(sprintf("#### %s \n", pName)) # Tabs header
source("_SCRIPTS/script_SINGLE.R")}
}else{
ops.j <- read.csv2(ops.j.path)
cat("Running Offline. Last available data from file:\n")
cat("\t", ops.j.path)
}
}
# Useful lists
poolId_list <- unique(liq.df$poolId) #all unique poolId
poolNames_list <- unique(liq.df$poolName) #unique poolNames
if(run_api){
for (x in 1:length(poolId_list)){
#POOL-NAME
id <- poolId_list[x]
pName <- poolNames_list[x]
cat(sprintf("#### %s \n", pName)) # Tabs header
source("_SCRIPTS/script_SINGLE.R")}
}else{
ops.j <- read.csv2(ops.j.path)
cat("Running Offline. Last available data from file:\n")
cat("\t", ops.j.path)
}
}
# Useful lists
poolId_list <- unique(liq.df$poolId) #all unique poolId
poolNames_list <- unique(liq.df$poolName) #unique poolNames
if(run_api){
for (x in 1:length(poolId_list)){
#POOL-NAME
id <- poolId_list[x]
pName <- poolNames_list[x]
cat(sprintf("#### %s \n", pName)) # Tabs header
source("_SCRIPTS/script_SINGLE.R")}
}else{
#  ops.table.path
#ops.j <- read.csv2(ops.table.path)
#cat("Running Offline. Last available data from file:\n")
#cat("\t", ops.table.path)
}
liq.df
#RUN OPTIONS
run_api = TRUE #toggle TRUE/FALSE to skip some parts of this script
downloadSubset = TRUE
useSubset = TRUE
subset_list <- c("BTC") #can be used to restrict analysis to a specific sibset of poolNames/Coins. Use REGEX syntax.
run_analysis = TRUE
run_plots = TRUE
refCoin = "USDT" #reference coin to express prices
#If not installed yet, install pacman package manager by "uncommenting" the line below
#install.packages("pacman") #this is only required on first run.
source("_SCRIPTS/init_externalLibs.R")
#If not installed yet, pacman will download and install all required packages on the first run of the script.
source("_SCRIPTS/init_functions.R")
source("_SCRIPTS/init_plots.R")
source("_SCRIPTS/init_dirs.R")
#API address
API_root <- "https://api.binance.com"
myPrivatePath <- "D:/Clouds/Dropbox/Everywhere/PROJECTS/PiggyBank/Binance/liquid-plot_myCredential.txt" #TEMP
#Import credentials
#keys <- read.csv2("credentials.txt", col.names = c("Type", "Key"), header = F)
keys_import <- read.csv2(myPrivatePath, col.names = c("Type", "Key"), header = F)
key_public <- keys_import[1,"Key", drop=T]
key_private <- keys_import[2,"Key", drop=T]
rm("keys_import")
#USER FEEDBACK
if(key_public=="YOUR-BINANCE-API-KEY"){
stop("STOP: add your Binance API credential to /credential.txt (; separated)")}
# DOWNLOAD PRICES
price.j.path <- paste0(dir.IN, "API_price.json")
price.csv.path <- paste0(dir.IN, "API_price_unpacked.csv")
if(run_api){
API_query <- "/api/v3/ticker/price"
price.j <- BINANCE.GET(API_root, "/api/v3/ticker/price", output = "both") #download Binance latest Prices
request <- price.j[[1]]
json <- price.j[[2]]
write_json(json, price.j.path, auto_unbox=TRUE) #export
cat("\t-> saved to: ", price.j.path, "\n")
price.df <- fromJSON(price.j.path, simplifyVector = TRUE,
flatten = TRUE) #automatically UN-Nest nested columns
#append price datastamp (from file last edit)
timestamp <- request$date %>% as.POSIXct(tz="UTC")
price.df[,"timestamp_UTC"] <- as.character(timestamp)
write.csv2(price.df, file = price.csv.path, row.names = F)
cat(sprintf("\tCONVERTED TO .CSV\t-> export to: %s (%s rows)\n", price.csv.path, nrow(price.df)))
}else{
price.df <- read.csv2(price.csv.path)
cat("Running Offline. Last available data from file:\n")
cat("\t", price.csv.path)
}
if(run_api){
# DOWNLOAD
liq.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/liquidity", timestamp = TRUE, sign = TRUE)
# JSON EXPORT
liq.j.path <- paste0(dir.IN, "API_liq.json")
write_json(liq.j, liq.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS
cat(sprintf("\t-> export to: %s\n", liq.j.path))
# TABLE INTERPRETATION
#> this function does not ONLY unpack liquidity data, but it also unpivot the table, to make it readable
liq.df <- liquidity.tableInterpreter(liq.j.path)
if(downloadSubset){
message("FILTER on DOWNLOAD: only selected pools are downloaded")            #TEMP
liq.df %<>% subset(grepl(subset_list, liq.df$poolName))}  #TEMP
liq.df.path <- paste0(dir.IN, "API_liquidity_unpacked.csv")
write.csv2(liq.df, liq.df.path, row.names = F)
cat(sprintf("\tUNPACKED INTO .CSV\t-> export to: %s (%s rows)\n", liq.df.path, nrow(liq.df)))
}else{
cat(sprintf("Running Offline. Last pool data were imported from file:\n\t%s\n", paste0(dir.IN,"API_liquidity_unpacked.csv")))
liq.df <- read.csv2(paste0(dir.IN,"API_liquidity_unpacked.csv"))}
if(useSubset){
message("FILTER on CALC: only selected pools are shown")
tot_pools <- nrow(liq.df)
liq.df %<>% subset(grepl(subset_list, liq.df$poolName))
cat(sprintf("\tAvailable: %s\tDisplayed: %s", tot_pools, nrow(liq.df)))
}
# FORMAT to PRINT
liq.df %>%
subset(select = c("poolName", "poolId", "share.Amount", "Date_UTC")) %>% #select useful col.
unique() %>% #filters out duplicated entries (2x for each pool)
kable(align= "c", digits = 2, caption = "<b>Currently ACTIVE Pools</b>") %>%
kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F) %>%
column_spec(c(1), bold=T)
liq.df
liq.df[, "Date_UTC"] <- liq.df$updateTime %>% msec_to_datetime()
liq.df[, "Date_UTC"] <- liq.df$updateTime %>% msec_to_datetime()
# FORMAT to PRINT
liq.df %>%
subset(select = c("poolName", "poolId", "share.Amount", "Date_UTC")) %>% #select useful col.
unique() %>% #filters out duplicated entries (2x for each pool)
kable(align= "c", digits = 2, caption = "<b>Currently ACTIVE Pools</b>") %>%
kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F) %>%
column_spec(c(1), bold=T)
# Useful lists
poolId_list <- unique(liq.df$poolId) #all unique poolId
poolNames_list <- unique(liq.df$poolName) #unique poolNames
for (x in 1:length(poolId_list)){
#POOL-NAME
id <- poolId_list[x]
pName <- poolNames_list[x]
cat(sprintf("#### %s\n", pName)) # Tabs header
cat("<div> \n") #Separate block for the TAB function
cat(sprintf("<p>Pool: %s (poolId=%s)</p>\n", pName, id))
# DATA ANALYIS
source("_SCRIPTS/script_CALC_v0.R") # all CALC
source("_SCRIPTS/script_PLOT_v0.R") # all PLOTS
# PLOTS
cat("<div>\n") #Separate block for the TAB function
print(pw) #main figure
cat("</div>\n")
# Link to Binance pool
cat(sprintf('<p>Direct Link to Binance Pool: <a href="https://www.binance.com/en/swap/liquidity?poolId=%s">(%s)</a></p>\n', id, poolName))
cat("<div> \n")
cat("##### TABLES{.tabset} \n") # Tabs header
cat("###### Pool Operations\n") # Tabs header
pool_LAST
cat("###### Pool Claims\n")
claim_CALC
cat("###### Pool Endpoints\n")
end_DF
cat("</div> \n")
}
pool_LAST
claim_CALC
