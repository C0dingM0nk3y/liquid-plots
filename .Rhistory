# IMPORT and Convert column data to the appropriate type (double, POSIX...)
pool_H <- read.csv2(pool.history.path) #%>% as.tibble()
pool_H
pool.history.path
pool.history.path
pool.history.path
pool.history.path
# TO-DO
#> wrap into function
#> export to single file, including all data
if(run_analysis){
for(id in poolId_list){
cat(sprintf("Exporting (poolId=%s) to /TABLES/SinglePools/:", id))
# PATHS
ops.j.path <- paste0(dir.IN.single,id,"_ops.json")
ops.table.path <- paste0(dir.TABLES.single,id,"_ops.csv") #REMOVE after making sure there is conflict
claim.j.path <- paste0(dir.IN.single,id,"_claim.json")
claim.table.path <- paste0(dir.TABLES.single,id,"_claim.csv")
#snap.table.path <- paste0(dir.TABLES.single,id,"_snapshot.csv")
#snap.history.path <- paste0(dir.TABLES.single,id,"_snapshots.History.csv") #REMOVE after first run
pool.history.path <- paste0(dir.TABLES.single,id,"_pool.History.csv")
# CLAIMED DATA
claim_DF <- claim.tableInterpreter(claim.j.path)
if(is.data.frame(claim_DF)){
write.csv2(claim_DF, claim.table.path, row.names = F) # EXPORT
cat(sprintf("\t _claim.csv (%s rows)", nrow(claim_DF)))}
else{
cat("\tno claimed data (SKIP)")}
# INTEPRET ops table
ops_DF <- ops.tableInterpreter(ops.j.path)
write.csv2(ops_DF, ops.table.path, row.names = F) # EXPORT #REMOVE?
cat(sprintf("\t _ops.csv (%s rows)", nrow(ops_DF))) #REMOVE?
# Last Pool Balance (SNAPSHOT)
snap_DF <-  subset(active.DF, poolId==id)
#add missing columns before joining to ops table
snap_DF[,"shareAmount"] <- snap_DF[,"share.Amount"] #name change
snap_DF[,"operation"] <- "SNAPSHOT"
# Update/Crete HISTORY file (HISTORY)
if (file.exists(pool.history.path)){
pool_prev <- read.csv2(pool.history.path) #import prev history
#recalculate from Unix, that is always the safest option
#> reason: if the file was opened with Excel, it changes all data formats)
pool_prev[, "Date_UTC"] <- msec_to_datetime(pool_prev$Date_Unix, tz = "UTC")}
else{pool_prev <- snap_DF} #...or replace it with latest data
# JOIN and removes duplicates
common_cols <- intersect(colnames(ops_DF), colnames(pool_prev)) #why? because snap_DF has some extra cols with Values, and I want to drop them
pool_HIST <- bind_rows(list(ops_DF, #operations (OLD and NEW)
pool_prev[,common_cols], # OLD DATA (including both OPS and SNAPS)
snap_DF[,common_cols])) # LAST SNAPSHOT (always NEW)
pool_HIST %<>% unique() #removes duplicates
pool_HIST <- pool_HIST[order(pool_HIST$Date_Unix), ] #order chronologically
write.csv2(pool_HIST, pool.history.path, row.names = F) # EXPORT HIST file
cat(sprintf("\t _pool.History.csv (read:%s, write:%s rows)\n", nrow(pool_prev), nrow(pool_HIST)))
}
}
for (id in poolId_list[1]){
source("_SCRIPTS/script_poolPlot_v1.R") # all plot calc
}
source("D:/Clouds/GitHub/REPOS/liquid-plots/_SCRIPTS/extraModule_reflData.R", echo=TRUE)
source("_SCRIPTS/init_externalLibs.R")
source("D:/Clouds/GitHub/REPOS/liquid-plots/_SCRIPTS/extraModule_reflData.R", echo=TRUE)
if(run_api){
# DOWNLOAD
liq.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/liquidity", timestamp = TRUE, sign = TRUE)
# JSON EXPORT
liq.j.path <- paste0(dir.IN, "API_liq.json")
write_json(liq.j, liq.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS
cat(sprintf("\t-> export to: %s\n", liq.j.path))
# TABLE INTERPRETATION
#> this function does not ONLY unpack liquidity data, but it also unpivot the table, to make it readable
liq.df <- liquidity.tableInterpreter(liq.j.path)
if(subset_download){
message("FITER APPLIED: only selected pools are kept")            #TEMP
liq.df %<>% subset(grepl(subset_list, liq.df$poolName))}  #TEMP
liq.df.path <- paste0(dir.IN, "API_liquidity_unpacked.csv")
write.csv2(liq.df, liq.df.path, row.names = F)
cat(sprintf("\tUNPACKED INTO .CSV\t-> export to: %s (%s rows)\n", liq.df.path, nrow(liq.df)))
}else{cat("SKIPPED\n")}
#lastTimestamp <- last_claim[[100]]$claimedTime #recovers last claimed time [[100]] from last list [[n-1]]
lastTimestamp <- last_claim[[1]]$claimedTime #recovers last claimed time [[100]] from last list [[n-1]]
#RUN OPTIONS
#limit downlod to specific sublist (for troubleshooting purposes)
subset_list <- c("BTC|ETH") #use REGEX to filter pools by their names
#> this is used in combination with subset_download or subset_analysis
#toggle TRUE/FALSE to skip some parts of this script
run_api = TRUE # download data/user local
subset_download = FALSE #FALSE = use full data
run_analysis = TRUE
subset_analysis = FALSE #FALSE = use full data
#keys.path <- "credentials.txt" #default is "credential.txt", change if desired
keys.path <-"D:/Clouds/Dropbox/Everywhere/PROJECTS/PiggyBank/Binance/liquid-plot_myCredential.txt"
#ANALYSIS OPTIONS
setStopLoss <- 0.005 #set desired stop loss for plots
takeProfit <- 0 #> from -0.99 to 0.99: threshhold for take profit (%)
#> 0 = disabled, >0 take profit on coin 1, <0 take profit on coin 2
allow_refData = FALSE #allow the use of externally provided data. Currently disabled.
#If not installed yet, install pacman package manager by "uncommenting" the line below
#install.packages("pacman") #this is only required on first run.
source("_SCRIPTS/init_externalLibs.R")
#If not installed yet, pacman will download and install all required packages on the first run of the script.
source("_SCRIPTS/init_functions.R")
source("_SCRIPTS/init_plots.R")
source("_SCRIPTS/init_dirs.R")
#API address
API_root <- "https://api.binance.com"
if(run_api){
#Import credentials
keys_import <- read_lines(keys.path, n_max = 2)
key_public <- keys_import[1]
key_private <- keys_import[2]
rm("keys_import")
#USER FEEDBACK
if(key_public=="YOUR-BINANCE-API-KEY"){
stop("STOP: add your Binance API credential to /credential.txt. \nFormat:\n\tpublic-key\n\tprivate-key")
}
}
# DOWNLOAD PRICES
if(run_api){
API_query <- "/api/v3/ticker/price"
price.j.path <- paste0(dir.IN, "API_price.json")
price.j <- BINANCE.GET(API_root, "/api/v3/ticker/price") #download Binance latest Prices
write_json(price.j, price.j.path, auto_unbox=TRUE) #export
cat("\t-> saved to: ", price.j.path, "\n")
price.df <- fromJSON(price.j.path, simplifyVector = TRUE,
flatten = TRUE) #automatically UN-Nest nested columns
#append price datastamp (from file last edit)
timestamp <- file.info(price.j.path)$mtime %>% as.POSIXct(tz="UTC") %>%
round(0) #this is to remove msec from time
price.df[,"timestamp_UTC"] <- as.character(timestamp)
write.csv2(price.df, file = str_replace(price.j.path, pattern = ".json", "_unpacked.csv"), row.names = F)
cat(sprintf("\tCONVERTED TO .CSV\t-> export to: %s (%s rows)\n",
str_replace(price.j.path, pattern = ".json", "_unpacked.csv"), nrow(price.df)))
}else{cat("SKIPPED\n")}
if(run_api){
# DOWNLOAD
liq.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/liquidity", timestamp = TRUE, sign = TRUE)
# JSON EXPORT
liq.j.path <- paste0(dir.IN, "API_liq.json")
write_json(liq.j, liq.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS
cat(sprintf("\t-> export to: %s\n", liq.j.path))
# TABLE INTERPRETATION
#> this function does not ONLY unpack liquidity data, but it also unpivot the table, to make it readable
liq.df <- liquidity.tableInterpreter(liq.j.path)
if(subset_download){
message("FITER APPLIED: only selected pools are kept")            #TEMP
liq.df %<>% subset(grepl(subset_list, liq.df$poolName))}  #TEMP
liq.df.path <- paste0(dir.IN, "API_liquidity_unpacked.csv")
write.csv2(liq.df, liq.df.path, row.names = F)
cat(sprintf("\tUNPACKED INTO .CSV\t-> export to: %s (%s rows)\n", liq.df.path, nrow(liq.df)))
}else{cat("SKIPPED\n")}
if(run_api){
liq.df[,"Date_UTC"] <- msec_to_datetime(liq.df$updateTime)
#Print current Pools
liq.df %>%
subset(select = c("poolName", "poolId", "Date_UTC", "share.Amount")) %>% #select useful col.
unique() %>% #filters out duplicated entries (2x for each pool)
kable(align= "c", caption = "<b>Currently ACTIVE Pools</b>") %>%
kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F) %>%
column_spec(c(1), bold=T)
}else{cat("SKIPPED\n")}
# MAX trx x pool is 100. Use pagination if more is required (may be required for Claims)
max_iter = 10 #so far, only applies to Claimed. operations does not requires that
if(run_api){
poolId_list <- unique(liq.df$poolId) #all unique poolId
poolNames_list <- unique(liq.df$poolName) #unique poolNames
for (n in 1:length(poolId_list)){
#POOL-NAME
id <- poolId_list[n]
pName <- poolNames_list[n]
cat(sprintf("\n\tPool: %s (poolId=%s)\n", pName, id))
#POOL OPERATIONS
ops.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/liquidityOps",
API_param = paste0("limit=100&poolId=",id),
timestamp = TRUE, sign = TRUE)
ops.j.path <- paste0(dir.IN.single, id, "_ops.json")
write_json(ops.j, ops.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS/SinglePools/
#CLAIMED DATA
claim.j_list <- list() #for data collection
# first ITER
claim.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/claimedHistory",
API_param = paste0("type=1&limit=100&poolId=",id), #type 0/1 = pending/successful
timestamp = TRUE, sign = TRUE)
claim.j_list[1] <- list(claim.j) #claim.j is already a list (1:100). This step creates a list of list.
# more ITER, if required
for (n in 1:max_iter){
if (max_iter==0){break} #allow to break loop in case this is desirable (e.g. quick update)
last_claim <- claim.j_list[[n]]
if (length(last_claim)==100){ #that means that data reached limit (100) and need a separate API call to prev. one
#lastTimestamp <- last_claim[[100]]$claimedTime #recovers last claimed time [[100]] from last list [[n-1]]
lastTimestamp <- last_claim[[1]]$claimedTime #recovers last claimed time [[100]] from last list [[n-1]]
cat(paste0("\titer=",n+1,"\t")) #add an indentation and iter number to export
claim.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/claimedHistory", timestamp = TRUE, sign = TRUE,
API_param = paste0("type=1&limit=100&poolId=",id, "&endTime=", lastTimestamp))
#Why lastTimestamp and NOT lastTimestamp-1
# > each time stamps has 2-3 rows (one for each coin + extra rewards)
# > there is 2/3 chance that last row of table is incomplete, and miss at least one of the coin.
# > therefore, it is better to download that again, and the remove duplicate entries in later steps
claim.j_list[n+1] <- list(claim.j) #list of list (JSON is 1-100 list)
}
else{break} #break loop: last claim had less than 100 entries
}
# List JOIN and EXPORT
claim.joined <- unlist(claim.j_list, recursive = F)
claim.j.path <- paste0(dir.IN.single, id, "_claim.json")
write_json(claim.joined, claim.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS/SinglePools/
if (length(claim.joined)>100){cat(sprintf("\t\t joined %s entries into single file: %s\n", length(claim.joined), claim.j.path))}
}
}else{cat("SKIPPED\n")}
last_claim <- claim.j_list[[n]]
last_claim
1:max_iter
#POOL-NAME
id <- poolId_list[n]
id
id=33
#POOL-NAME
id <- poolId_list[n]
pName <- poolNames_list[n]
cat(sprintf("\n\tPool: %s (poolId=%s)\n", pName, id))
#POOL OPERATIONS
ops.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/liquidityOps",
API_param = paste0("limit=100&poolId=",id),
timestamp = TRUE, sign = TRUE)
#POOL OPERATIONS
ops.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/liquidityOps",
API_param = paste0("limit=100&poolId=",id),
timestamp = TRUE, sign = TRUE)
ops.j.path <- paste0(dir.IN.single, id, "_ops.json")
ops.j.path <- paste0(dir.IN.single, id, "_ops.json")
write_json(ops.j, ops.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS/SinglePools/
#CLAIMED DATA
claim.j_list <- list() #for data collection
# first ITER
claim.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/claimedHistory",
API_param = paste0("type=1&limit=100&poolId=",id), #type 0/1 = pending/successful
timestamp = TRUE, sign = TRUE)
claim.j_list[1] <- list(claim.j) #claim.j is already a list (1:100). This step creates a list of list.
# more ITER, if required
for (n in 1:max_iter){
if (max_iter==0){break} #allow to break loop in case this is desirable (e.g. quick update)
last_claim <- claim.j_list[[n]]
if (length(last_claim)==100){ #that means that data reached limit (100) and need a separate API call to prev. one
lastTimestamp <- last_claim[[100]]$claimedTime #recovers last claimed time [[100]] from last list [[n-1]]
#lastTimestamp <- last_claim[[1]]$claimedTime #recovers last claimed time [[100]] from last list [[n-1]]
cat(paste0("\titer=",n+1,"\t")) #add an indentation and iter number to export
claim.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/claimedHistory", timestamp = TRUE, sign = TRUE,
API_param = paste0("type=1&limit=100&poolId=",id, "&endTime=", lastTimestamp))
#Why lastTimestamp and NOT lastTimestamp-1
# > each time stamps has 2-3 rows (one for each coin + extra rewards)
# > there is 2/3 chance that last row of table is incomplete, and miss at least one of the coin.
# > therefore, it is better to download that again, and the remove duplicate entries in later steps
claim.j_list[n+1] <- list(claim.j) #list of list (JSON is 1-100 list)
}
else{break} #break loop: last claim had less than 100 entries
}
claim.j
pName="test"
pName <- poolNames_list[n]
cat(sprintf("\n\tPool: %s (poolId=%s)\n", pName, id))
pName <- poolNames_list[n]
#POOL-NAME
id <- poolId_list[n]
pName <- poolNames_list[n]
cat(sprintf("\n\tPool: %s (poolId=%s)\n", pName, id))
#POOL OPERATIONS
ops.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/liquidityOps",
API_param = paste0("limit=100&poolId=",id),
timestamp = TRUE, sign = TRUE)
ops.j.path <- paste0(dir.IN.single, id, "_ops.json")
ops.j.path
id=33
#POOL OPERATIONS
ops.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/liquidityOps",
API_param = paste0("limit=100&poolId=",id),
timestamp = TRUE, sign = TRUE)
ops.j.path <- paste0(dir.IN.single, id, "_ops.json")
ops.j.path
#CLAIMED DATA
claim.j_list <- list() #for data collection
# first ITER
claim.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/claimedHistory",
API_param = paste0("type=1&limit=100&poolId=",id), #type 0/1 = pending/successful
timestamp = TRUE, sign = TRUE)
claim.j
lastTimestamp <- last_claim[[100]]$claimedTime #recovers last claimed time [[100]] from last list [[n-1]]
lastTimestamp <- last_claim$claimedTime %>% tail(1)
n=1
last_claim <- claim.j_list[[n]]
last_claim
last_claim <- claim.j_list[[n]]
claim.j_list[1] <- list(claim.j) #claim.j is already a list (1:100). This step creates a list of list.
last_claim <- claim.j_list[[n]]
last_claim
lastTimestamp <- last_claim[[100]]$claimedTime #recovers last claimed time [[100]] from last list [[n-1]]
lastTimestamp <- last_claim$claimedTime %>% tail(1)
lastTimestamp
last_claim$claimedTime
last_claim
last_claim <- claim.j_list[[n]]
lastTimestamp <- last_claim[[100]]$claimedTime #recovers last claimed time [[100]] from last list [[n-1]]
lastTimestamp <- last_claim$claimedTime %>% tail(1)
lastTimestamp
lastTimestamp <- last_claim
last_claim
# first ITER
claim.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/claimedHistory",
API_param = paste0("type=1&limit=100&poolId=",id), #type 0/1 = pending/successful
timestamp = TRUE, sign = TRUE)
claim.j
# MAX trx x pool is 100. Use pagination if more is required (may be required for Claims)
max_iter = 10 #so far, only applies to Claimed. operations does not requires that
if(run_api){
poolId_list <- unique(liq.df$poolId) #all unique poolId
poolNames_list <- unique(liq.df$poolName) #unique poolNames
for (n in 1:length(poolId_list)){
#POOL-NAME
id <- poolId_list[n]
pName <- poolNames_list[n]
cat(sprintf("\n\tPool: %s (poolId=%s)\n", pName, id))
#POOL OPERATIONS
ops.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/liquidityOps",
API_param = paste0("limit=100&poolId=",id),
timestamp = TRUE, sign = TRUE)
ops.j.path <- paste0(dir.IN.single, id, "_ops.json")
write_json(ops.j, ops.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS/SinglePools/
#CLAIMED DATA
claim.j_list <- list() #for data collection
# first ITER
claim.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/claimedHistory",
API_param = paste0("type=1&limit=100&poolId=",id), #type 0/1 = pending/successful
timestamp = TRUE, sign = TRUE)
claim.j_list[1] <- list(claim.j) #claim.j is already a list (1:100). This step creates a list of list.
# more ITER, if required
for (n in 1:max_iter){
if (max_iter==0){break} #allow to break loop in case this is desirable (e.g. quick update)
last_claim <- claim.j_list[[n]]
#> changed from ==100 to >90. Reason: some exports from Binance have less than 100 entries, despite tehre are many more in the past.
if (length(last_claim)==90){ #that means that data reached limit (100) and need a separate API call to prev. one
lastTimestamp <- last_claim[[length(last_claim)]]$claimedTime #recovers last claimed time [[100]] from last list [[n-1]]
#> PREV version
#if (length(last_claim)==90){ #that means that data reached limit (100) and need a separate API call to prev. one
#lastTimestamp <- last_claim[[100]]$claimedTime #recovers last claimed time [[100]] from last list [[n-1]]
cat(paste0("\titer=",n+1,"\t")) #add an indentation and iter number to export
claim.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/claimedHistory", timestamp = TRUE, sign = TRUE,
API_param = paste0("type=1&limit=100&poolId=",id, "&endTime=", lastTimestamp))
#Why lastTimestamp and NOT lastTimestamp-1
# > each time stamps has 2-3 rows (one for each coin + extra rewards)
# > there is 2/3 chance that last row of table is incomplete, and miss at least one of the coin.
# > therefore, it is better to download that again, and the remove duplicate entries in later steps
claim.j_list[n+1] <- list(claim.j) #list of list (JSON is 1-100 list)
}
else{break} #break loop: last claim had less than 100 entries
}
# List JOIN and EXPORT
claim.joined <- unlist(claim.j_list, recursive = F)
claim.j.path <- paste0(dir.IN.single, id, "_claim.json")
write_json(claim.joined, claim.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS/SinglePools/
if (length(claim.joined)>100){cat(sprintf("\t\t joined %s entries into single file: %s\n", length(claim.joined), claim.j.path))}
}
}else{cat("SKIPPED\n")}
# MAX trx x pool is 100. Use pagination if more is required (may be required for Claims)
max_iter = 10 #so far, only applies to Claimed. operations does not requires that
if(run_api){
poolId_list <- unique(liq.df$poolId) #all unique poolId
poolNames_list <- unique(liq.df$poolName) #unique poolNames
for (n in 1:length(poolId_list)){
#POOL-NAME
id <- poolId_list[n]
pName <- poolNames_list[n]
cat(sprintf("\n\tPool: %s (poolId=%s)\n", pName, id))
#POOL OPERATIONS
ops.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/liquidityOps",
API_param = paste0("limit=100&poolId=",id),
timestamp = TRUE, sign = TRUE)
ops.j.path <- paste0(dir.IN.single, id, "_ops.json")
write_json(ops.j, ops.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS/SinglePools/
#CLAIMED DATA
claim.j_list <- list() #for data collection
# first ITER
claim.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/claimedHistory",
API_param = paste0("type=1&limit=100&poolId=",id), #type 0/1 = pending/successful
timestamp = TRUE, sign = TRUE)
claim.j_list[1] <- list(claim.j) #claim.j is already a list (1:100). This step creates a list of list.
# more ITER, if required
for (n in 1:max_iter){
if (max_iter==0){break} #allow to break loop in case this is desirable (e.g. quick update)
last_claim <- claim.j_list[[n]]
#> changed from ==100 to >90. Reason: some exports from Binance have less than 100 entries, despite tehre are many more in the past.
if (length(last_claim)>90){ #that means that data reached limit (100) and need a separate API call to prev. one
lastTimestamp <- last_claim[[length(last_claim)]]$claimedTime #recovers last claimed time [[100]] from last list [[n-1]]
#> PREV version
#if (length(last_claim)==90){ #that means that data reached limit (100) and need a separate API call to prev. one
#lastTimestamp <- last_claim[[100]]$claimedTime #recovers last claimed time [[100]] from last list [[n-1]]
cat(paste0("\titer=",n+1,"\t")) #add an indentation and iter number to export
claim.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/claimedHistory", timestamp = TRUE, sign = TRUE,
API_param = paste0("type=1&limit=100&poolId=",id, "&endTime=", lastTimestamp))
#Why lastTimestamp and NOT lastTimestamp-1
# > each time stamps has 2-3 rows (one for each coin + extra rewards)
# > there is 2/3 chance that last row of table is incomplete, and miss at least one of the coin.
# > therefore, it is better to download that again, and the remove duplicate entries in later steps
claim.j_list[n+1] <- list(claim.j) #list of list (JSON is 1-100 list)
}
else{break} #break loop: last claim had less than 100 entries
}
# List JOIN and EXPORT
claim.joined <- unlist(claim.j_list, recursive = F)
claim.j.path <- paste0(dir.IN.single, id, "_claim.json")
write_json(claim.joined, claim.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS/SinglePools/
if (length(claim.joined)>100){cat(sprintf("\t\t joined %s entries into single file: %s\n", length(claim.joined), claim.j.path))}
}
}else{cat("SKIPPED\n")}
# VARIABLES and DF that need to be re-imported before running Part 2
if(run_api==FALSE){
if (!(file.exists(paste0(dir.IN,"API_price_unpacked.csv")))){
stop("Missing file: ", paste0(dir.IN,"API_price_unpacked.csv", "  -> select run_api=TRUE"))}
if (!(file.exists(paste0(dir.IN,"API_liquidity_unpacked.csv")))){
stop("Missing file: ", paste0(dir.IN,"API_liquidity_unpacked.csv", "  -> select run_api=TRUE"))}
price.df <- read.csv2(paste0(dir.IN,"API_price_unpacked.csv"))
liq.df <- read.csv2(paste0(dir.IN,"API_liquidity_unpacked.csv"))
if(subset_analysis){
message("FILTER APPLIED: only selected pools are kept")
liq.df %<>% subset(grepl(subset_list, liq.df$poolName))}
}
# Useful lists
poolId_list <- unique(liq.df$poolId) #all unique poolId
poolNames_list <- unique(liq.df$poolName) #unique poolNames
# unique coins (to fetch prices)
coinList <- poolNames_list %>% str_split("/") %>% unlist() %>%
c("BNB") %>% #add BNB, as this will be needed for extraRewards calculations
unique()
refCoin = "USDT"
if(run_analysis){
# EXTRACTS PRICES for the relevant coins
priceMatrix <- data.frame()
for (c in coinList){
priceMatrix[c,refCoin] <- getPrice(price.df, c, refCoin = refCoin) %>% as.numeric()}
priceMatrix
}else{cat("SKIPPED\n")}
#> Unpivot active-pools
#> Append current prices
#> Calculate values
if(run_analysis){
# Assign price value to each coin
liq.df[,"Price"] <- priceMatrix[liq.df$Coin, refCoin] #recover price from priceMatrix
liq.df[,"Value"] <- with(liq.df, share.asset*Price) %>% round(8)
liq.df[,"Currency"] <- refCoin
# perfrom all CALCULATION in separate FUNC
active.DF <- activePools.Calc(liq.df)
# EXPORT
write.csv2(active.DF, paste0(dir.TABLES, "ActivePools.csv"), row.names = F)
# USER PLOT
# Print current Pools
active.DF %>% subset(select = c("poolName", "poolId", "Date_UTC",
"Qnt1", "Coin1", "Qnt2", "Coin2",
"Value_TOT", "Currency")) %>% #select useful col.
kable(digits = c(2,2,2,4,4,4,4,2,2), align= c("c","c","r","r","l","r","l","r","l"),
caption = "<b>Currently ACTIVE Pools</b>") %>%
kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F) %>%
column_spec(c(4,5), color="darkblue") %>%
column_spec(c(1,8,9), bold=T)
}else{cat("SKIPPED\n")}
# TO-DO
#> wrap into function
#> export to single file, including all data
if(run_analysis){
for(id in poolId_list){
cat(sprintf("Exporting (poolId=%s) to /TABLES/SinglePools/:", id))
# PATHS
ops.j.path <- paste0(dir.IN.single,id,"_ops.json")
ops.table.path <- paste0(dir.TABLES.single,id,"_ops.csv") #REMOVE after making sure there is conflict
claim.j.path <- paste0(dir.IN.single,id,"_claim.json")
claim.table.path <- paste0(dir.TABLES.single,id,"_claim.csv")
#snap.table.path <- paste0(dir.TABLES.single,id,"_snapshot.csv")
#snap.history.path <- paste0(dir.TABLES.single,id,"_snapshots.History.csv") #REMOVE after first run
pool.history.path <- paste0(dir.TABLES.single,id,"_pool.History.csv")
# CLAIMED DATA
claim_DF <- claim.tableInterpreter(claim.j.path)
if(is.data.frame(claim_DF)){
write.csv2(claim_DF, claim.table.path, row.names = F) # EXPORT
cat(sprintf("\t _claim.csv (%s rows)", nrow(claim_DF)))}
else{
cat("\tno claimed data (SKIP)")}
# INTEPRET ops table
ops_DF <- ops.tableInterpreter(ops.j.path)
write.csv2(ops_DF, ops.table.path, row.names = F) # EXPORT #REMOVE?
cat(sprintf("\t _ops.csv (%s rows)", nrow(ops_DF))) #REMOVE?
# Last Pool Balance (SNAPSHOT)
snap_DF <-  subset(active.DF, poolId==id)
#add missing columns before joining to ops table
snap_DF[,"shareAmount"] <- snap_DF[,"share.Amount"] #name change
snap_DF[,"operation"] <- "SNAPSHOT"
# Update/Crete HISTORY file (HISTORY)
if (file.exists(pool.history.path)){
pool_prev <- read.csv2(pool.history.path) #import prev history
#recalculate from Unix, that is always the safest option
#> reason: if the file was opened with Excel, it changes all data formats)
pool_prev[, "Date_UTC"] <- msec_to_datetime(pool_prev$Date_Unix, tz = "UTC")}
else{pool_prev <- snap_DF} #...or replace it with latest data
# JOIN and removes duplicates
common_cols <- intersect(colnames(ops_DF), colnames(pool_prev)) #why? because snap_DF has some extra cols with Values, and I want to drop them
pool_HIST <- bind_rows(list(ops_DF, #operations (OLD and NEW)
pool_prev[,common_cols], # OLD DATA (including both OPS and SNAPS)
snap_DF[,common_cols])) # LAST SNAPSHOT (always NEW)
pool_HIST %<>% unique() #removes duplicates
pool_HIST <- pool_HIST[order(pool_HIST$Date_Unix), ] #order chronologically
write.csv2(pool_HIST, pool.history.path, row.names = F) # EXPORT HIST file
cat(sprintf("\t _pool.History.csv (read:%s, write:%s rows)\n", nrow(pool_prev), nrow(pool_HIST)))
}
}
for (id in poolId_list){
source("_SCRIPTS/script_poolPlot_v1.R") # all plot calc
}
