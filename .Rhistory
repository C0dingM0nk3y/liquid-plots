# INITIALIZE (Packages + standard PATHS)
source("D:/Clouds/Dropbox/Everywhere/PROJECTS/PiggyBank/Binance/@SCRIPTS/Binance_INIT_v1.R")
source(paste0(dir.Scripts,"/Binance_FUNCTIONS_v1.9.R"))
binanceURL <- "https://api.binance.com"
key <- 'BFpq7Yvwx3XjwFZVwG2QsI2MU41z83f7HFt4zse4AY1TYYnYzxedT4wU9ECwfDg8'
prKey <- '6GcFrUj7fsF1O1D9ViBFTCy6dFNGHP4aodREZD0wYffndFVsRTnGccqSbjrK83j1'
#> v 1.1: changed API_list path to @SCRIPTS
API.list <- read.csv2(file=paste0(dir.Scripts,"API_list_v2.1.csv")) %>% print()
APIverbose = FALSE
source("Binance_WalletUpdate_MARKET_v1.R")
source("Binance_WalletUpdate_MARKET_v1.R")
priceEUR_DF
# INITIALIZE (Packages + standard PATHS)
source("D:/Clouds/Dropbox/Everywhere/PROJECTS/PiggyBank/Binance/@SCRIPTS/Binance_INIT_v1.R")
source(paste0(dir.Scripts,"/Binance_FUNCTIONS_v1.9.R"))
binanceURL <- "https://api.binance.com"
key <- 'BFpq7Yvwx3XjwFZVwG2QsI2MU41z83f7HFt4zse4AY1TYYnYzxedT4wU9ECwfDg8'
prKey <- '6GcFrUj7fsF1O1D9ViBFTCy6dFNGHP4aodREZD0wYffndFVsRTnGccqSbjrK83j1'
#> v 1.1: changed API_list path to @SCRIPTS
API.list <- read.csv2(file=paste0(dir.Scripts,"API_list_v2.1.csv")) %>% print()
APIverbose = FALSE
source("Binance_WalletUpdate_MARKET_v1.R")
priceEUR_DF
source("Binance_WalletUpdate_SPOT_v1.3.R")
spot_DF
source("Binance_WalletUpdate_STACK_v1.4.R")
#comments: so far renewwable == FALSE -> go back to safe (data: CAKE)
#comments: so far renewwable == NA -> go back to safe (data: AAVE)
stack_DF
stack_lastWeek_DF #%>% data.table::as.data.table()
# Filtering option (applied in LP3)
updateAllSegments <- FALSE
maxSegments <- 2
skipOlderThan_days <- 10 #time in days (from closing time)
skiplist <- c("") #temp workaround # REMOVE
source("Binance_WalletUpdate_LP1_Data_v1.5.R")
source("Binance_WalletUpdate_LP2_Analysis_v1.3.R") #COMPLETE POOL/COIN Summaries
#source("D:/Clouds/Dropbox/Everywhere/PROJECTS/PiggyBank/Binance/@SCRIPTS/Binance_WalletUpdate_v1/Binance_WalletUpdate_LP3_Segments_v1.R") #SEGMENT Analysis
source("Binance_WalletUpdate_LP3_Segments_v1.7.R") #SEGMENT Analysis
#> note on calculation
#> whenever I add/remove coins from a pool, I am changing the proportion of pool I possess
#> It is possible to calculate WHAT IS LEFT in the pool, by applying a simple proportion
#> First, calculate the new portion value
#> Portion NOW [PortionTOT] = Portion START + Portion Change
#> Then, calculate the proportion of Coin1 and Coin2 to the Portion change
#> Finally, calculate the Qnt of Coin1/2 in Portion NOW
#Why do I normalize over the OPEN amount of Coin?
#> because my plots show IL% as losses compared to HODL.
#> And HODL is calculated over the Pool EntryPrice (pool OPEN).
#> So to be able to meaningfully overlap the 2 %, they should refer to the same TP (pool OPEN)
#> Future research: do I get the same % if I calculate over VALUE, instead than COINS?
LPsnap_DF
LPsnap_DF2
source("Binance_WalletUpdate_WALLET_v2.R")
WALLET_DF[order(WALLET_DF$Binance_Value, decreasing = T),]
W_CHANGES_DF
#> NOTES
#> It seems that some LD assets are only updated AFTER 11:00. In fact, running again the SPOT module, make them visbile again
#> This suggests that the problen can be solved from the SPOT module
#> 1. Change module so to calc the whole spot history, day by day (1-7). Then shoot warning in case of big changes.
#> 2. Recover missing data by using the dedicated SAVE API?
#> 3. Leave the W_Changes_DF as a final visualization tool for coin movements (e.g. from SPOT to SAVE to LP...)
source("Binance_WalletUpdate_CMC_v1.R")
runAPIgroup(API.list, "Fiat", APIverbose = F)
#runAPIgroup(API.list, "Offer", APIverbose = F)
# INITIALIZE (Packages + standard PATHS)
source("D:/Clouds/Dropbox/Everywhere/PROJECTS/PiggyBank/Binance/@SCRIPTS/Binance_INIT_v1.R")
source(paste0(dir.Scripts,"/Binance_FUNCTIONS_v1.9.R"))
binanceURL <- "https://api.binance.com"
key <- 'BFpq7Yvwx3XjwFZVwG2QsI2MU41z83f7HFt4zse4AY1TYYnYzxedT4wU9ECwfDg8'
prKey <- '6GcFrUj7fsF1O1D9ViBFTCy6dFNGHP4aodREZD0wYffndFVsRTnGccqSbjrK83j1'
#> v 1.1: changed API_list path to @SCRIPTS
API.list <- read.csv2(file=paste0(dir.Scripts,"API_list_v2.1.csv")) %>% print()
APIverbose = FALSE
source("Binance_WalletUpdate_MARKET_v1.R")
priceEUR_DF
source("Binance_WalletUpdate_SPOT_v1.3.R")
spot_DF
source("Binance_WalletUpdate_STACK_v1.4.R")
#comments: so far renewwable == FALSE -> go back to safe (data: CAKE)
#comments: so far renewwable == NA -> go back to safe (data: AAVE)
stack_DF
stack_lastWeek_DF #%>% data.table::as.data.table()
# Filtering option (applied in LP3)
updateAllSegments <- FALSE
maxSegments <- 2
skipOlderThan_days <- 10 #time in days (from closing time)
skiplist <- c("") #temp workaround # REMOVE
source("Binance_WalletUpdate_LP1_Data_v1.5.R")
source("Binance_WalletUpdate_LP2_Analysis_v1.3.R") #COMPLETE POOL/COIN Summaries
#source("D:/Clouds/Dropbox/Everywhere/PROJECTS/PiggyBank/Binance/@SCRIPTS/Binance_WalletUpdate_v1/Binance_WalletUpdate_LP3_Segments_v1.R") #SEGMENT Analysis
source("Binance_WalletUpdate_LP3_Segments_v1.7.R") #SEGMENT Analysis
#> note on calculation
#> whenever I add/remove coins from a pool, I am changing the proportion of pool I possess
#> It is possible to calculate WHAT IS LEFT in the pool, by applying a simple proportion
#> First, calculate the new portion value
#> Portion NOW [PortionTOT] = Portion START + Portion Change
#> Then, calculate the proportion of Coin1 and Coin2 to the Portion change
#> Finally, calculate the Qnt of Coin1/2 in Portion NOW
#Why do I normalize over the OPEN amount of Coin?
#> because my plots show IL% as losses compared to HODL.
#> And HODL is calculated over the Pool EntryPrice (pool OPEN).
#> So to be able to meaningfully overlap the 2 %, they should refer to the same TP (pool OPEN)
#> Future research: do I get the same % if I calculate over VALUE, instead than COINS?
LPsnap_DF
LPsnap_DF2
source("Binance_WalletUpdate_WALLET_v2.R")
WALLET_DF[order(WALLET_DF$Binance_Value, decreasing = T),]
W_CHANGES_DF
#> NOTES
#> It seems that some LD assets are only updated AFTER 11:00. In fact, running again the SPOT module, make them visbile again
#> This suggests that the problen can be solved from the SPOT module
#> 1. Change module so to calc the whole spot history, day by day (1-7). Then shoot warning in case of big changes.
#> 2. Recover missing data by using the dedicated SAVE API?
#> 3. Leave the W_Changes_DF as a final visualization tool for coin movements (e.g. from SPOT to SAVE to LP...)
source("Binance_WalletUpdate_CMC_v1.R")
source("Binance_WalletUpdate_CMC_v1.R")
source("Binance_WalletUpdate_CMC_v1.R")
#RUN OPTIONS
#limit downlod to specific sublist (for troubleshooting purposes)
subset_list <- c("BTC|ETH") #use REGEX to filter pools by their names
#> this is used in combination with subset_download or subset_analysis
#toggle TRUE/FALSE to skip some parts of this script
run_api = TRUE # download data/user local
subset_download = FALSE #FALSE = use full data
run_analysis = TRUE
subset_analysis = FALSE #FALSE = use full data
#keys.path <- "credentials.txt" #default is "credential.txt", change if desired
keys.path <-"D:/Clouds/Dropbox/Everywhere/PROJECTS/PiggyBank/Binance/liquid-plot_myCredential.txt"
#ANALYSIS OPTIONS
setStopLoss <- 0.005 #set desired stop loss for plots
takeProfit <- 0 #> from -0.99 to 0.99: threshhold for take profit (%)
#> 0 = disabled, >0 take profit on coin 1, <0 take profit on coin 2
allow_refData = FALSE #allow the use of externally provided data. Currently disabled.
id
for (id in poolId_list[1]){
source("_SCRIPTS/script_poolPlot_v1.R") # all plot calc
}
# Useful lists
poolId_list <- unique(liq.df$poolId) #all unique poolId
poolNames_list <- unique(liq.df$poolName) #unique poolNames
#RUN OPTIONS
#limit downlod to specific sublist (for troubleshooting purposes)
subset_list <- c("BTC|ETH") #use REGEX to filter pools by their names
#> this is used in combination with subset_download or subset_analysis
#toggle TRUE/FALSE to skip some parts of this script
run_api = FALSE # download data/user local
subset_download = FALSE #FALSE = use full data
run_analysis = TRUE
subset_analysis = FALSE #FALSE = use full data
#keys.path <- "credentials.txt" #default is "credential.txt", change if desired
keys.path <-"D:/Clouds/Dropbox/Everywhere/PROJECTS/PiggyBank/Binance/liquid-plot_myCredential.txt"
#ANALYSIS OPTIONS
setStopLoss <- 0.005 #set desired stop loss for plots
takeProfit <- 0 #> from -0.99 to 0.99: threshhold for take profit (%)
#> 0 = disabled, >0 take profit on coin 1, <0 take profit on coin 2
allow_refData = FALSE #allow the use of externally provided data. Currently disabled.
#If not installed yet, install pacman package manager by "uncommenting" the line below
#install.packages("pacman") #this is only required on first run.
source("_SCRIPTS/init_externalLibs.R")
#If not installed yet, pacman will download and install all required packages on the first run of the script.
source("_SCRIPTS/init_functions.R")
source("_SCRIPTS/init_plots.R")
source("_SCRIPTS/init_dirs.R")
#API address
API_root <- "https://api.binance.com"
if(run_api){
#Import credentials
keys_import <- read_lines(keys.path, n_max = 2)
key_public <- keys_import[1]
key_private <- keys_import[2]
rm("keys_import")
#USER FEEDBACK
if(key_public=="YOUR-BINANCE-API-KEY"){
stop("STOP: add your Binance API credential to /credential.txt. \nFormat:\n\tpublic-key\n\tprivate-key")
}
}
# DOWNLOAD PRICES
if(run_api){
API_query <- "/api/v3/ticker/price"
price.j.path <- paste0(dir.IN, "API_price.json")
price.j <- BINANCE.GET(API_root, "/api/v3/ticker/price") #download Binance latest Prices
write_json(price.j, price.j.path, auto_unbox=TRUE) #export
cat("\t-> saved to: ", price.j.path, "\n")
price.df <- fromJSON(price.j.path, simplifyVector = TRUE,
flatten = TRUE) #automatically UN-Nest nested columns
#append price datastamp (from file last edit)
timestamp <- file.info(price.j.path)$mtime %>% as.POSIXct(tz="UTC") %>%
round(0) #this is to remove msec from time
price.df[,"timestamp_UTC"] <- as.character(timestamp)
write.csv2(price.df, file = str_replace(price.j.path, pattern = ".json", "_unpacked.csv"), row.names = F)
cat(sprintf("\tCONVERTED TO .CSV\t-> export to: %s (%s rows)\n",
str_replace(price.j.path, pattern = ".json", "_unpacked.csv"), nrow(price.df)))
}else{cat("SKIPPED\n")}
if(run_api){
# DOWNLOAD
liq.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/liquidity", timestamp = TRUE, sign = TRUE)
# JSON EXPORT
liq.j.path <- paste0(dir.IN, "API_liq.json")
write_json(liq.j, liq.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS
cat(sprintf("\t-> export to: %s\n", liq.j.path))
# TABLE INTERPRETATION
#> this function does not ONLY unpack liquidity data, but it also unpivot the table, to make it readable
liq.df <- liquidity.tableInterpreter(liq.j.path)
if(subset_download){
message("FITER APPLIED: only selected pools are kept")            #TEMP
liq.df %<>% subset(grepl(subset_list, liq.df$poolName))}  #TEMP
liq.df.path <- paste0(dir.IN, "API_liquidity_unpacked.csv")
write.csv2(liq.df, liq.df.path, row.names = F)
cat(sprintf("\tUNPACKED INTO .CSV\t-> export to: %s (%s rows)\n", liq.df.path, nrow(liq.df)))
}else{cat("SKIPPED\n")}
if(run_api){
liq.df[,"Date_UTC"] <- msec_to_datetime(liq.df$updateTime)
#Print current Pools
liq.df %>%
subset(select = c("poolName", "poolId", "Date_UTC", "share.Amount")) %>% #select useful col.
unique() %>% #filters out duplicated entries (2x for each pool)
kable(align= "c", caption = "<b>Currently ACTIVE Pools</b>") %>%
kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F) %>%
column_spec(c(1), bold=T)
}else{cat("SKIPPED\n")}
# MAX trx x pool is 100. Use pagination if more is required (may be required for Claims)
max_iter = 10 #so far, only applies to Claimed. operations does not requires that
if(run_api){
poolId_list <- unique(liq.df$poolId) #all unique poolId
poolNames_list <- unique(liq.df$poolName) #unique poolNames
for (n in 1:length(poolId_list)){
#POOL-NAME
id <- poolId_list[n]
pName <- poolNames_list[n]
cat(sprintf("\n\tPool: %s (poolId=%s)\n", pName, id))
#POOL OPERATIONS
ops.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/liquidityOps",
API_param = paste0("limit=100&poolId=",id),
timestamp = TRUE, sign = TRUE)
ops.j.path <- paste0(dir.IN.single, id, "_ops.json")
write_json(ops.j, ops.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS/SinglePools/
#CLAIMED DATA
claim.j_list <- list() #for data collection
# first ITER
claim.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/claimedHistory",
API_param = paste0("type=1&limit=100&poolId=",id), #type 0/1 = pending/successful
timestamp = TRUE, sign = TRUE)
claim.j_list[1] <- list(claim.j) #claim.j is already a list (1:100). This step creates a list of list.
# more ITER, if required
for (n in 1:max_iter){
if (max_iter==0){break} #allow to break loop in case this is desirable (e.g. quick update)
last_claim <- claim.j_list[[n]]
if (length(last_claim)==100){ #that means that data reached limit (100) and need a separate API call to prev. one
lastTimestamp <- last_claim[[100]]$claimedTime #recovers last claimed time [[100]] from last list [[n-1]]
cat(paste0("\titer=",n+1,"\t")) #add an indentation and iter number to export
claim.j <- BINANCE.GET(API_root, "/sapi/v1/bswap/claimedHistory", timestamp = TRUE, sign = TRUE,
API_param = paste0("type=1&limit=100&poolId=",id, "&endTime=", lastTimestamp))
#Why lastTimestamp and NOT lastTimestamp-1
# > each time stamps has 2-3 rows (one for each coin + extra rewards)
# > there is 2/3 chance that last row of table is incomplete, and miss at least one of the coin.
# > therefore, it is better to download that again, and the remove duplicate entries in later steps
claim.j_list[n+1] <- list(claim.j) #list of list (JSON is 1-100 list)
}
else{break} #break loop: last claim had less than 100 entries
}
# List JOIN and EXPORT
claim.joined <- unlist(claim.j_list, recursive = F)
claim.j.path <- paste0(dir.IN.single, id, "_claim.json")
write_json(claim.joined, claim.j.path, auto_unbox=TRUE) # export JSON file to /DOWNLOADS/SinglePools/
if (length(claim.joined)>100){cat(sprintf("\t\t joined %s entries into single file: %s\n", length(claim.joined), claim.j.path))}
}
}else{cat("SKIPPED\n")}
# Useful lists
poolId_list <- unique(liq.df$poolId) #all unique poolId
poolNames_list <- unique(liq.df$poolName) #unique poolNames
# VARIABLES and DF that need to be re-imported before running Part 2
if(run_api==FALSE){
if (!(file.exists(paste0(dir.IN,"API_price_unpacked.csv")))){
stop("Missing file: ", paste0(dir.IN,"API_price_unpacked.csv", "  -> select run_api=TRUE"))}
if (!(file.exists(paste0(dir.IN,"API_liquidity_unpacked.csv")))){
stop("Missing file: ", paste0(dir.IN,"API_liquidity_unpacked.csv", "  -> select run_api=TRUE"))}
price.df <- read.csv2(paste0(dir.IN,"API_price_unpacked.csv"))
liq.df <- read.csv2(paste0(dir.IN,"API_liquidity_unpacked.csv"))
if(subset_analysis){
message("FILTER APPLIED: only selected pools are kept")
liq.df %<>% subset(grepl(subset_list, liq.df$poolName))}
}
# Useful lists
poolId_list <- unique(liq.df$poolId) #all unique poolId
poolNames_list <- unique(liq.df$poolName) #unique poolNames
# unique coins (to fetch prices)
coinList <- poolNames_list %>% str_split("/") %>% unlist() %>%
c("BNB") %>% #add BNB, as this will be needed for extraRewards calculations
unique()
poolId_list
for (id in poolId_list[1]){
source("_SCRIPTS/script_poolPlot_v1.R") # all plot calc
}
refCoin = "USDT"
if(run_analysis){
# EXTRACTS PRICES for the relevant coins
priceMatrix <- data.frame()
for (c in coinList){
priceMatrix[c,refCoin] <- getPrice(price.df, c, refCoin = refCoin) %>% as.numeric()}
priceMatrix
}else{cat("SKIPPED\n")}
#> Unpivot active-pools
#> Append current prices
#> Calculate values
if(run_analysis){
# Assign price value to each coin
liq.df[,"Price"] <- priceMatrix[liq.df$Coin, refCoin] #recover price from priceMatrix
liq.df[,"Value"] <- with(liq.df, share.asset*Price) %>% round(8)
liq.df[,"Currency"] <- refCoin
# perfrom all CALCULATION in separate FUNC
active.DF <- activePools.Calc(liq.df)
# EXPORT
write.csv2(active.DF, paste0(dir.TABLES, "ActivePools.csv"), row.names = F)
# USER PLOT
# Print current Pools
active.DF %>% subset(select = c("poolName", "poolId", "Date_UTC",
"Qnt1", "Coin1", "Qnt2", "Coin2",
"Value_TOT", "Currency")) %>% #select useful col.
kable(digits = c(2,2,2,4,4,4,4,2,2), align= c("c","c","r","r","l","r","l","r","l"),
caption = "<b>Currently ACTIVE Pools</b>") %>%
kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F) %>%
column_spec(c(4,5), color="darkblue") %>%
column_spec(c(1,8,9), bold=T)
}else{cat("SKIPPED\n")}
for(id in poolId_list[1]){
cat(sprintf("Exporting (poolId=%s) to /TABLES/SinglePools/:", id))
# PATHS
ops.j.path <- paste0(dir.IN.single,id,"_ops.json")
ops.table.path <- paste0(dir.TABLES.single,id,"_ops.csv") #REMOVE after making sure there is conflict
claim.j.path <- paste0(dir.IN.single,id,"_claim.json")
claim.table.path <- paste0(dir.TABLES.single,id,"_claim.csv")
#snap.table.path <- paste0(dir.TABLES.single,id,"_snapshot.csv")
#snap.history.path <- paste0(dir.TABLES.single,id,"_snapshots.History.csv") #REMOVE after first run
pool.history.path <- paste0(dir.TABLES.single,id,"_pool.History.csv")
# CLAIMED DATA
claim_DF <- claim.tableInterpreter(claim.j.path)
if(is.data.frame(claim_DF)){
write.csv2(claim_DF, claim.table.path, row.names = F) # EXPORT
cat(sprintf("\t _claim.csv (%s rows)", nrow(claim_DF)))}
else{
cat("\tno claimed data (SKIP)")}
# INTEPRET ops table
ops_DF <- ops.tableInterpreter(ops.j.path)
write.csv2(ops_DF, ops.table.path, row.names = F) # EXPORT #REMOVE?
cat(sprintf("\t _ops.csv (%s rows)", nrow(ops_DF))) #REMOVE?
# Last Pool Balance (SNAPSHOT)
snap_DF <-  subset(active.DF, poolId==id)
#add missing columns before joining to ops table
snap_DF[,"shareAmount"] <- snap_DF[,"share.Amount"] #name change
snap_DF[,"operation"] <- "SNAPSHOT"
# Update/Crete HISTORY file (HISTORY)
if (file.exists(pool.history.path)){
pool_prev <- read.csv2(pool.history.path) #import prev history
#recalculate from Unix, that is always the safest option
#> reason: if the file was opened with Excel, it changes all data formats)
pool_prev[, "Date_UTC"] <- msec_to_datetime(pool_prev$Date_Unix, tz = "UTC")}
else{pool_prev <- snap_DF} #...or replace it with latest data
# JOIN and removes duplicates
common_cols <- intersect(colnames(ops_DF), colnames(pool_prev)) #why? because snap_DF has some extra cols with Values, and I want to drop them
pool_HIST <- bind_rows(list(ops_DF, #operations (OLD and NEW)
pool_prev[,common_cols], # OLD DATA (including both OPS and SNAPS)
snap_DF[,common_cols])) # LAST SNAPSHOT (always NEW)
pool_HIST %<>% unique() #removes duplicates
pool_HIST <- pool_HIST[order(pool_HIST$Date_Unix), ] #order chronologically
write.csv2(pool_HIST, pool.history.path, row.names = F) # EXPORT HIST file
cat(sprintf("\t _pool.History.csv (read:%s, write:%s rows)\n", nrow(pool_prev), nrow(pool_HIST)))
}
# PATHS
ops.j.path <- paste0(dir.IN.single,id,"_ops.json")
ops.table.path <- paste0(dir.TABLES.single,id,"_ops.csv") #REMOVE after making sure there is conflict
claim.j.path <- paste0(dir.IN.single,id,"_claim.json")
claim.table.path <- paste0(dir.TABLES.single,id,"_claim.csv")
source("_SCRIPTS/script_poolPlot_v1.R") # all plot calc
setwd("D:/Clouds/GitHub/liquid-plots")
# Rebuild PATHS
claim.table.path <- paste0(dir.TABLES.single,id,"_claim.csv")
pool.history.path <- paste0(dir.TABLES.single,id,"_pool.History.csv") #includes OPS and SNAPS
refData.path <- paste0(dir.REF.single,id,"_refData.csv")
claim.table.path
pool.history.path
refData.path
# IMPORT and Convert column data to the appropriate type (double, POSIX...)
pool_H <- read.csv2(pool.history.path) #%>% as.tibble()
pool_H[,"Date_UTC"] %<>% as.POSIXct(tz="UTC")
if (file.exists(claim.table.path)){
claim_H <- read.csv2(claim.table.path) #REPLACE WITH HIST FILE
claim_H[,"Date_UTC"] %<>% as.POSIXct(tz="UTC")
}else{
claim_H <- data.frame(Date_UTC = now(tzone = "UTC"), #mock Dataframe
claimed1 = 0,
claimed2= 0,
claimed3 = 0,
Coin3=NA)
}
## 2.A Pool INFO ####
poolName <- pool_H[1,"poolName", drop=T]
coin1 <- pool_H[1,"Coin1", drop=T]
poolName
pool_H
pool.history.path
# IMPORT and Convert column data to the appropriate type (double, POSIX...)
pool_H <- read.csv2(pool.history.path) #%>% as.tibble()
pool_H
pool.history.path <- paste0(dir.TABLES.single,id,"_pool.History.csv") #includes OPS and SNAPS
pool.history.path
# IMPORT and Convert column data to the appropriate type (double, POSIX...)
pool_H <- read.csv2(pool.history.path) #%>% as.tibble()
pool_H
pool.history.path
pool.history.path
pool.history.path
pool.history.path
# TO-DO
#> wrap into function
#> export to single file, including all data
if(run_analysis){
for(id in poolId_list){
cat(sprintf("Exporting (poolId=%s) to /TABLES/SinglePools/:", id))
# PATHS
ops.j.path <- paste0(dir.IN.single,id,"_ops.json")
ops.table.path <- paste0(dir.TABLES.single,id,"_ops.csv") #REMOVE after making sure there is conflict
claim.j.path <- paste0(dir.IN.single,id,"_claim.json")
claim.table.path <- paste0(dir.TABLES.single,id,"_claim.csv")
#snap.table.path <- paste0(dir.TABLES.single,id,"_snapshot.csv")
#snap.history.path <- paste0(dir.TABLES.single,id,"_snapshots.History.csv") #REMOVE after first run
pool.history.path <- paste0(dir.TABLES.single,id,"_pool.History.csv")
# CLAIMED DATA
claim_DF <- claim.tableInterpreter(claim.j.path)
if(is.data.frame(claim_DF)){
write.csv2(claim_DF, claim.table.path, row.names = F) # EXPORT
cat(sprintf("\t _claim.csv (%s rows)", nrow(claim_DF)))}
else{
cat("\tno claimed data (SKIP)")}
# INTEPRET ops table
ops_DF <- ops.tableInterpreter(ops.j.path)
write.csv2(ops_DF, ops.table.path, row.names = F) # EXPORT #REMOVE?
cat(sprintf("\t _ops.csv (%s rows)", nrow(ops_DF))) #REMOVE?
# Last Pool Balance (SNAPSHOT)
snap_DF <-  subset(active.DF, poolId==id)
#add missing columns before joining to ops table
snap_DF[,"shareAmount"] <- snap_DF[,"share.Amount"] #name change
snap_DF[,"operation"] <- "SNAPSHOT"
# Update/Crete HISTORY file (HISTORY)
if (file.exists(pool.history.path)){
pool_prev <- read.csv2(pool.history.path) #import prev history
#recalculate from Unix, that is always the safest option
#> reason: if the file was opened with Excel, it changes all data formats)
pool_prev[, "Date_UTC"] <- msec_to_datetime(pool_prev$Date_Unix, tz = "UTC")}
else{pool_prev <- snap_DF} #...or replace it with latest data
# JOIN and removes duplicates
common_cols <- intersect(colnames(ops_DF), colnames(pool_prev)) #why? because snap_DF has some extra cols with Values, and I want to drop them
pool_HIST <- bind_rows(list(ops_DF, #operations (OLD and NEW)
pool_prev[,common_cols], # OLD DATA (including both OPS and SNAPS)
snap_DF[,common_cols])) # LAST SNAPSHOT (always NEW)
pool_HIST %<>% unique() #removes duplicates
pool_HIST <- pool_HIST[order(pool_HIST$Date_Unix), ] #order chronologically
write.csv2(pool_HIST, pool.history.path, row.names = F) # EXPORT HIST file
cat(sprintf("\t _pool.History.csv (read:%s, write:%s rows)\n", nrow(pool_prev), nrow(pool_HIST)))
}
}
for (id in poolId_list[1]){
source("_SCRIPTS/script_poolPlot_v1.R") # all plot calc
}
